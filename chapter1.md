# 팀 마르코브 게임에서 최적의 내쉬 균형을 플레이하는 강화학습

```
Xiaofeng Wang

ECE Department Carnegie Mellon University 
Pittsburgh, PA 15213 
xiaofeng@andrew.cmu.edu

Tuomas Sandholm

CS Department Carnegie Mellon University 
Pittsburgh, PA 15213 
sandholm@cs.cmu.edu
```

##Abstract

멀티 에이전트 학습은 A.I.의 핵심 문제입니다. 몇 십년간, 컴퓨터 과학자들은 강화학습의 영역을 멀티 에이전트 설정에서 풀어내려고 노력해왔습니다. 마르코프 게임이 일반적인 멀티 에이전트 강화학습 모델로서 부상했습니다. Nash-Q라는 이름의 접근법이 여러 에이전트의 전략들을 결정하는 구조로 제안이 되었습니다. 만약에 내쉬 균형이 존재할 때, Nash-Q는 수렴합니다. 하지만, 일반적으로는 여러개의 내쉬 균형이 존재합니다. 심지어 (여러 에이전트가 같은 목적을 갖고 있는) 팀 마르코프 게임에서도 여러 개의 내쉬 균형이 있을 수 있을지라도, 그것들 중 일부만 최적입니다. 그러므로, 이러한 설정에서의 학습은 그리 중요하지 않습니다.

이 문제에 대한 직접적인 해결책은 협약 (사회 법)을 시행하는 것입니다. Boutilier는 에이전트가 사전 작업 순서로 개별 작업을 선택하는 타이 브레이킹 스키마를 제안했습니다 [1]. 그러나 디자이너가 컨벤션을 부과 할 수 없거나 원치 않는 많은 설정이 있습니다. 이러한 경우 상담원은 조정을 배워야합니다. Claus와 Boutilier는 RL에 게임 이론에서의 평형 선택 기법 인 가상 놀이를 소개했습니다. 그들의 알고리즘 인 JAL (joint action learner) [2]은 팀 스테이지 게임에서 내쉬 균형에 대한 수렴을 보장합니다. 그러나이 평형은 최적이 아닐 수도 있습니다. 같은 문제가 적응 형 놀이 [18]와 [7]에서 제안 된 진화 모델과 같은 게임 이론에서의 다른 평형 선택 접근법에 우선합니다.

RL에서 에이전트는 일반적으로 환경 모델 (게임)을 알지 못하고 noisy한 보상을 받는다. 이 경우 에이전트가 독립적으로 noisy한 보상을 받아 tie를 인식하지 못하기 때문에 사전식 접근 방식조차도 작동하지 않을 수 있습니다. 또 다른 중요한 이전의 연구에서의 문제는 대리인이 항상 최선의 반응을 보이거나 일정한 비율로 실수를 한다는 가정하에 연구 된 평형 선택 접근법의 수렴에 RL이 요구하는 비정적 탐사 정책이 어떻게 영향을 미치는지에 있습니다. RL에서, 팀 Markov 게임에서 최적의 Nash 평형을 학습하는 것이 중요한 열린 문제 중 하나로 제기되었다. 이 프로 브램에 대한 발견 적 접근이 있었지만,이 설정에서 최적의 내쉬 균형으로 수렴 할 수있는 기존 알고리즘은 제안되지 않았습니다.
본 논문에서는 팀 마르코프 게임 (3 절)에서 확률 1로 최적의 내쉬 균형으로 수렴하는 최초의 알고리즘 인 최적 적응 학습 (OAL)을 제시한다. 우리는 컨버전스를 증명하고, OAL의 매개 변수가 컨버전스 조건을 쉽게 만족한다는 것을 보여줍니다 (4 절).