Deep Mean Field Games for Learning Optimal Behavior Policy of
Large Populations

# Abstract

이산 상태 공간에 대한 인구 분포의 진화를 이끌어내는 거대한 인구 행동 정책을 나타내는 문제를 고려한다. 이산 시간 평균 필드 게임 (MFG)은 개별 행동의 총 효과를 이해하고 인구 분포의 시간적 변화를 예측하기 위해 게임 이론에 기초한 해석 가능한 모델로 동기 부여됩니다. 특수 MFG가 MDP로 축소 될 수 있음을 보여줌으로써 MFG 및 Markov 결정 프로세스 (MDP)의 합성을 달성합니다. 이를 통해 우리는 평균 역학 이론의 범위를 확대하고 심층 역 보강 학습을 통해 대형 실제 시스템의 MFG 모델을 추론 할 수 있습니다. 우리의 방법은 실제 데이터에서 MFG의 보상 기능과 순방향 역학을 학습하고 실제 소셜 미디어 인구의 평균 현장 게임 모델에 대한 첫 번째 경험적 테스트를보고합니다.

We consider the problem of representing a large population’s behavior policy that drives the evolution of the population distribution over a discrete state space. A discrete time mean field game (MFG) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions. We achieve a synthesis of MFG and Markov decision processes (MDP) by showing that a special MFG is reducible to an MDP. This enables us to broaden the scope of mean field game theory and infer MFG models of large real-world systems via deep inverse reinforcement learning. Our method learns both the reward function and forward dynamics of an MFG from real data, and we report the first empirical test of a mean field game model of a real-world social media population.

# 1 Introduction

의미가 최대 또는 최소의 의미가 아닌 세상에서는 아무 것도 일어나지 않습니다. - (레온하르트 오일러)

Nothing takes place in the world whose meaning is not that of some maximum or minimum. - (Leonhard Euler)

아랍계 봄철 시위의 물결, 블랙 라이프 스터즈 운동, 2016 년 미국 대통령 선거에서의 가짜 뉴스 논란과 같은 대규모 인구를 포함한 주요 글로벌 사건은 거시적 인 인구 행태를 설명하는 새로운 모델을 고안하는 데 중요한 자극을 제공합니다 (Howard et al., 2011; Anderson and Hitlin, 2016; Silverman, 2016). 오일러의 진술이 암시하는 최소한의 행동 원칙에 따라 물리적 시스템이 행동하는 것처럼, 개별 행동에서 나오는 인구 행동 또한 일부 목표와 관련하여 최적 일 수 있습니다. 영향력있는 역할
현대 대중 운동에서 소셜 미디어의 중요성은이 가설에 대한 타당성을 제공한다. (Perrin, 2015) 정보의 가용성은 개인이 전세계 인구 상태에 대한 그들의 관찰에 기초하여 계획하고 행동 할 수있게하기 때문에 가능하다. 예를 들어, 인구의 행동은 토픽에 대한 세계 인구 분포로 대표되는 소셜 미디어에 대한 트렌드 주제의 순위에 직접적으로 영향을 미치는 반면,이 글로벌 상태에 대한 각 사용자의 관찰은 참여할 다음 주제의 선택에 영향을줍니다. 미래의 인구 행동에 기여합니다 (Twitter, 2017).
일반적으로이 현상은 한 집단에 대한 많은 인구의 분포가 집단 자체에 의해 관찰 가능 (또는 부분적으로 관찰 가능)하고, 암묵적 행동 정책이 관찰에 의해 통보되는 모든 체계에 존재한다. 이것은 인구 행동 모델에 대한 여러 기준에 동기를 부여한다.

1. 모델은 인구 분포와 행동 정책 간의 의존성을 포착한다.
2. 모든 개인의 총체적인 결정에 의해 최적화 된 보상의 개념을 통해 설명 가능하다.
3. 이전 시간에 측정 된 상태 공간에 대한 미래의 분포를 예측할 수 있으며 실제 데이터로부터 학습 할 수 있습니다.

우리는 모델링 및 예측 기준을 다루기위한 Mean Field Game (MFG) 접근 방식을 제시합니다. 평균 필드 게임은 N이 무한대로 변할 때 N 플레이어 게임의 한계를 고려하여 대형 에이전트 집단의 다루기 쉬운 모델을 제공하는 게임 이론의 한 부분으로 시작되었습니다 (Lasry and Lions, 2007). 이 한계에서, 에이전트 공간은 상태 공간에 대한 분포를 통해 표현되며, 각 에이전트 간의 상호 영향은 극소가되며, 각 에이전트의 최적 전략은 인구 분포 및 집계 조치의 함수 인 보상에 의해 알려 집니 다. 가장 일반적인 형태로, MFG는 경제적 자원의 생산을 모델화하기 위해 전문화 될 수있는 확률 미분 방정식의 한 계급을 대표한다 (Gueant 'et al., 2011), 소셜 네트워크에서의 의견 역학 (Bauso et al.
2016), 소비자 집단에 의한 경쟁 기술의 채택 (Lachapelle et al., 2010). 대리인을 유통으로 대표하는 것은 MFG가 임의의 인구 규모로 확장 가능하므로 경기장에서 멕시코 파도와 같은 실제 현상을 시뮬레이션 할 수 있습니다 (Gueant '외., 2011).
3 장에서 상세히 설명 된 모델이 보여 주듯이, MFG는 대안 적 예측 방법의 한계를 극복함으로써 우리의 문제 환경에서 모델링 기준을 자연스럽게 해결합니다. 예를 들어, 시계열 분석은 데이터로부터 예측 모델을 구축하지만, 이러한 모델은 보상 기능의 최적화 결과로 행동을 고려하지 않기 때문에 인구 행동 정책을 생성하는 동기에 대한 통찰력을 제공하지 못할 수 있습니다. 대안으로, 기본 모집단 네트워크 구조를 사용하는 방법은 노드가 로컬 이웃에 의해서만 영향을 받고, 글로벌 상태의 표현을 포함하지 않으며 제어되지 않은 암시 적 최적화의 결과로 이벤트를 설명하는 데 어려움을 겪을 수 있다고 가정했습니다 (Farajtabar 외 ., 2015; De et al., 2016). MFG는 시스템이 근본적인 최적 제어 정책에 따라 자연스럽게 작동하는 방법을 알려주는 설명 모델로서 독특합니다. 이
는 마르코프 결정 과정 (MDP)과 강화 학습 (RL)의 틀 (Sutton and Barto, 1998)과의 연결을 가능하게하는 필수적인 통찰력입니다. 기존의 MDP 관점과의 중요한 차이점은 MDP 정책 최적화를 통해 MFG 모델 추론으로 문제를 프레임 화한다는 것입니다. 시스템을 외부 적으로 제어하지 않고 관련 MDP를 해결함으로써 시스템이 자체적으로 수행하는 암시 적 최적화를 추정합니다.
MFG는 복잡한 보상 기능을 배우기 위해 유연한 신경망을 함수 근사기로 사용하여 역 보강 학습 (IRL) 방법 (Ng and Russell, 2000, Ziebart et al., 2008, Finn et al., 2016) 그것은 임의로 큰 집단의 행동을 설명합니다. 다른 방향에서 RL은 실제 시스템의 MFG 모델을 해결하기위한 데이터 기반 방법을 고안 할 수있게합니다. 최근 몇 년 동안 MFG 이론에 대한 연구가 급속히 진행되었지만 합성 장난감 문제의 수치 시뮬레이션에 대한 몇 가지 사례를 통해 경험적 검증을위한 확장 가능한 방법의 부재가 눈에 띄지 않습니다 (Lachapelle et al., 2010; Achdou et al., 2012 Bauso et al., 2016). 따라서 우리는 MFG가 인구 행동 모델링의 특정 문제에 어떻게 잘 맞는지 보여 주지만 MFG 및 MDP의 합성을 통해 MFG 추론에 대한 일반적인 데이터 중심 접근법을 입증합니다.
우리의 주요 공헌은 다음과 같습니다. 우리는 보상 기능과 함께 MFG 모델을 학습하는 데이터 중심 접근 방식을 제안합니다. MFG의 연구가 인위적인 보상 기능을 갖춘 완구 문제에만 국한 될 필요는 없음을 보여줍니다.
특히, 우리는 일반적인 MFG로부터 이산 시간 그래프 - 상태 MFG를 도출하고 실제 환경에서 세부적인 해석을 제공합니다 (3 절). 그 다음에 우리는 특별한 경우가 MDP로 축소 될 수 있음을 증명하고, MDP에서 최적의 정책 및 보상 기능을 찾는 것이 MFG 모델의 추론과 동등하다는 것을 증명한다 (4 절). 우리의 접근법을 사용하여 우리는 소셜 미디어에서의 인구 분포의 MFG 모델을 경험적으로 검증합니다 (5 절).
학습 된 MFG 모델은 기준선에 비해 예측 성능이 훨씬 뛰어나고 인구 행동에 대한 통찰력을 제공합니다. MDP와의 MFG 합성은 두 분야의 새로운 연구 방향을 제시 할 잠재력이 있습니다.

Major global events involving large populations, such as the wave of protests during the Arab Spring, the Black Lives Matter movement, and the controversy over fake news during the 2016 U.S. presidential election, provide significant impetus for devising new models that account for macroscopic population behavior resulting from the aggregation of decisions and actions taken by all individuals (Howard et al., 2011; Anderson and Hitlin, 2016; Silverman, 2016). Just as physical systems behave according to the principle of least action, to which Euler’s statement alludes, population behavior emerging from individual actions may also be optimal with respect to some objective. The influential role
of social media in modern mass movements lends plausibility to this hypothesis (Perrin, 2015), since the availability of information enables individuals to plan and act based on their observations of the global population state. For example, a population’s behavior directly affects the ranking of a set of trending topics on social media, represented by the global population distribution over topics, while each user’s observation of this global state influences their choice of the next topic in which to participate, thereby contributing to future population behavior (Twitter, 2017).
In general, this phenomenon is present in any system where the distribution of a large population over a set of states is observable (or partially observable) by the population itself, whose implicit behavior policy is informed by their observations. This motivates multiple criteria for a model of population behavior:
1. The model captures the dependency between population distribution and their behavior policy.
2. It is explainable via a notion of a reward optimized by the aggregate decisions of all individuals.
3. It enables prediction of future distribution over a state space given measurements at previous times, and can be learned from real data.


We present a mean field game (MFG) approach to address the modeling and prediction criteria. Mean field games originated as a branch of game theory that provides tractable models of large agent populations, by considering the limit of N-player games as N tends to infinity (Lasry and Lions, 2007). In this limit, an agent population is represented via their distribution over a state space, the mutual influence between individual agents becomes infinitesimal, and each agent’s optimal strategy is informed by a reward that is a function of the population distribution and their aggregate actions. In its most general form, MFG represents a class of stochastic differential equations that can be specialized to model the production of economic resources (Gueant ´ et al., 2011), opinion dynamics in social networks (Bauso et al.,
2016), and the adoption of competing technologies by consumer populations (Lachapelle et al., 2010). Representing agents as a distribution means that MFG is scalable to arbitrary population sizes, enabling it to simulate real-world phenomenon such as the Mexican wave in stadiums (Gueant ´ et al., 2011).
As the model detailed in Section 3 will show, MFG naturally addresses the modeling criteria in our problem context by overcoming limitations of alternative predictive methods. For example, time series analysis builds predictive models from data, but these models may not provide insight into the motivations that produce a population’s behavior policy, since they do not consider the behavior as the result of optimization of a reward function. Alternatively, methods that employ the underlying population network structure have assumed that nodes are only influenced by a local neighborhood, do not include a representation of a global state, and may face difficulty in explaining events as the result of uncontrolled implicit optimization (Farajtabar et al., 2015; De et al., 2016). MFG is unique as a descriptive model whose solution tells us how a system naturally behaves according to its underlying optimal control policy. This
is the essential insight that enables us to draw a connection with the framework of Markov decision processes (MDP) and reinforcement learning (RL) (Sutton and Barto, 1998). The crucial difference from a traditional MDP viewpoint is that we frame the problem as MFG model inference via MDP policy optimization: we infer the implicit optimization that the system performs on its own accord, by solving an associated MDP without externally controlling the system.
MFG offers a computationally tractable framework for adapting inverse reinforcement learning (IRL) methods (Ng and Russell, 2000; Ziebart et al., 2008; Finn et al., 2016), with flexible neural networks as function approximators, to learn complex reward functions that explain behavior of arbitrarily large populations. In the other direction, RL enables us to devise a data-driven method for solving an MFG model of a real-world system. While research on the theory of MFG has progressed rapidly in recent years, with some examples of numerical simulation of synthetic toy problems, there is a conspicuous absence of scalable methods for empirical validation (Lachapelle et al., 2010; Achdou et al., 2012; Bauso et al., 2016). Therefore, while we show how MFG is well-suited for the specific problem of modeling population behavior, we also demonstrate a general data-driven approach to MFG inference via a synthesis of MFG and MDP.
Our main contributions are the following. We propose a data-driven approach to learn an MFG model along with its reward function, showing that research in MFG need not be confined to toy problems with artificial reward functions.
Specifically, we derive a discrete time graph-state MFG from general MFG and provide detailed interpretation in a real-world setting (Section 3). Then we prove that a special case can be reduced to an MDP and show that finding an optimal policy and reward function in the MDP is equivalent to inference of the MFG model (Section 4). Using our approach, we empirically validate an MFG model of population’s activity distribution on social media (Section 5).
The learned MFG model shows significantly better predictive performance compared to baselines and offers insights on population behavior. Our synthesis of MFG with MDP has potential to open new research directions for both fields.

# 2. Related work

Mean Field 게임은 Lasry and Lions (2007)의 연구에서 유래되었으며, Huang 외에서는 독립적 인 확률 적 동적 게임으로 시작되었습니다. (2006)은 둘 다 경제 분야의 문제 모델링을위한 미분 방정식 형태의 평균 필드 문제를 제안하고 솔루션의 존재와 독창성을 분석했다. Gueant '외. (2011)는 MFG 모델에 대한 조사를 제공하고 우리의 작업에서 적용의 선택을 알리는 인구 분포 모델과 같은 지속적인 시공간적 인 다양한 응용 프로그램에 대해 논의했습니다. MFG 프레임 워크는 비용 함수 (즉, 부정적인 보상)의 선택에 대해 불가지론이지만, 선행 연구는 분석 솔루션을 달성하기 위해 비용에 대한 강한 가정을한다. 우리는 게임의 역 동성이 보상 기능에 크게 영향을 받는다는 견해를 가지고 있으며 따라서 우리는 데이터로부터 MFG 보상 기능을 학습하는 방법을 제안합니다.
시간과 공간에서의 MFG의 이산화 (Discretization of time and space)는 개별 주제에 대한 우리의 인구 분포 모형의 출발점으로 사용되는 Gomes et al., 2010; Achdou et al., 2012; 이러한 초기 작업은 솔루션 속성을 분석하고 경험적 검증이 부족하지만 실제 환경에서 솔루션을 얻는 알고리즘에 중점을 둡니다. 우리의 응용 사례와 관련하여, Bauso et al. (2016)은 다중 인구 환경에서 의견 역학의 진화를 분석했지만, 초기 인구 분포 및 에이전트 행동에 대한 제한에 대해 가우시안 밀도 가정을 부과했다. 둘 다 모델의 보편성을 제한하고 우리의 작업에서는 가정되지 않는다. 연속 평균 필드 게임을 해결하기위한 수치 유한 차분 방법에 대한 연구가있다 (Achdou et al., 2012; Lachapelle et al., 2010; Carlini and Silva, 2014). 이러한 방법은 초기화에 민감하고 고유 한 계산상의 어려움을 갖는 순방향 - 역방향 또는 뉴턴 반복을 포함합니다.
장난감 문제에 이러한 방법을 제한하고 크기를 조정할 수없는 큰 실수 값 상태 및 조치 공간의 경우
현실 세계의 문제. MFG 프레임 워크가 큰 실세계 영역에서 알려지지 않은 보상 기능을 포함하는 문제에 성공적이었던 RL 알고리즘을 어떻게 적용 할 수 있는지 보여줌으로써 이러한 한계를 극복했습니다.
보강 학습에서 큰 신경망을 큰 상태 및 행동 공간을 갖는 MDP를 해결하기위한 함수 근사자로 사용하는 수많은 가치 및 정책 기반 알고리즘이있다 (Mnih et al., 2013; Silver et al., 2014; Lillicrap et al. 2015). MDP와 Markov 게임 프레임 워크는 다중 에이전트 설정 (Hu 등, 1998, Littman, 2001, Lowe et al., 2017)에 대한 일반화가 있지만 수천 개의 상호 작용하는 에이전트를 포함하는 시스템을 표현하는 방법을 쉽게 제시하지 못합니다. 행동은 시간을 통해 최적의 탄도를 유도합니다. 우리의 연구에서 의미있는 필드 게임
이론은 RL이 적용될 수 있도록 모델링 문제를 구성하는 열쇠입니다.
역 보강 학습 (Ng and Russell, 2000)의 영역에서 최대 엔트로피 IRL 프레임 워크는 인간과 로봇 기관을 포함하는 상황에서 전문가 시위에서 알려지지 않은 보상 기능을 학습하는 데 성공적임을 입증했다 (Ziebart et al., 2008; Boularias et al. , 2011; Kalakrishnan et al., 2013). 이 확률 론적 프레임 워크는 데모 샘플 (Wulfmeier et al., 2015; Finn et al., 2016)에서 복잡한 보상 기능을 학습하기위한 심 신경 네트워크로 보완 될 수 있습니다. 우리의 MFG 모델은 Finn et al.에서 샘플 기반 IRL 알고리즘을 확장 할 수있게 해줍니다.
(2016)은 많은 사람들의 행동이 최적이되는 보상 기능을 학습하는 문제에 관한 것이며, 우리는 신경망을 사용하여 MFG 상태와 행동을 효율적으로 처리한다.

Mean field games originated in the work of Lasry and Lions (2007), and independently as stochastic dynamic games in Huang et al. (2006), both of which proposed mean field problems in the form of differential equations for modeling problems in economics and analyzed the existence and uniqueness of solutions. Gueant ´ et al. (2011) provided a survey of MFG models and discussed various applications in continuous time and space, such as a model of population distribution that informed the choice of application in our work. Even though the MFG framework is agnostic towards the choice of cost function (i.e. negative reward), prior work make strong assumptions on the cost in order to attain analytic solutions. We take a view that the dynamics of any game is heavily impacted by the reward function, and hence we propose methods to learn the MFG reward function from data.
Discretization of MFGs in time and space have been proposed (Gomes et al., 2010; Achdou et al., 2012; Gueant ´ , 2015), serving as the starting point for our model of population distribution over discrete topics; while these early work analyze solution properties and lack empirical verification, we focus on algorithms for attaining solutions in real-world settings. Related to our application case, prior work by Bauso et al. (2016) analyzed the evolution of opinion dynamics in multi-population environments, but they imposed a Gaussian density assumption on the initial population distribution and restrictions on agent actions, both of which limit the generality of the model and are not assumed in our work. There is a collection of work on numerical finite-difference methods for solving continuous mean field games (Achdou et al., 2012; Lachapelle et al., 2010; Carlini and Silva, 2014). These methods involve forward-backward or Newton iterations that are sensitive to initialization and have inherent computational challenges
for large real-valued state and action spaces, which limit these methods to toy problems and cannot be scaled to
real-world problems. We overcome these limitations by showing how the MFG framework enables adaptation of RL algorithms that have been successful for problems involving unknown reward functions in large real-world domains.
In reinforcement learning, there are numerous value- and policy-based algorithms employing deep neural networks as function approximators for solving MDPs with large state and action spaces (Mnih et al., 2013; Silver et al., 2014; Lillicrap et al., 2015). Even though there are generalizations to multi-agent settings (Hu et al., 1998; Littman, 2001; Lowe et al., 2017), the MDP and Markov game frameworks do not easily suggest how to represent systems involving thousands of interacting agents whose actions induce an optimal trajectory through time. In our work, mean field game
theory is the key to framing the modeling problem such that RL can be applied.
In the area of inverse reinforcement learning (Ng and Russell, 2000), the maximum entropy IRL framework has proved successful at learning unknown reward functions from expert demonstrations in situations involving human and robotic agency (Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013). This probabilistic framework can be augmented with deep neural networks for learning complex reward functions from demonstration samples (Wulfmeier et al., 2015; Finn et al., 2016). Our MFG model enables us to extend the sample -based IRL algorithm in Finn et al.
(2016) to the problem of learning a reward function under which a large population’s behavior is optimal, and we employ a neural network to process MFG states and actions efficiently.

# 3 Mean field games

우리는 그래프를 통한 연속적인 평균 필드 게임의 개요부터 시작하여 일반 이산 시간 그래프 상태 MFG (Gueant ', 2015)를 유도합니다. 그런 다음이 논문의 나머지 부분에 초점을 맞출 완전한 그래프에 대해 이산 시간 (discrete-time)의 MFG에 대한 자세한 프레젠테이션을 제공합니다.

We begin with an overview of a continuous-time mean field games over graphs, and derive a general discrete-time graph-state MFG (Gueant ´ , 2015). Then we give a detailed presentation of a discrete-time MFG over a complete graph, which will be the focus for the rest of this paper.