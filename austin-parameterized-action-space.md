# DEEPREINFORCEMENTLEARNING INPARAMETER-IZEDACTIONSPACE

Matthew Hausknecht

Department of Computer Science University of Texas at Austinmhauskn@cs.utexas.edu

Peter Stone

Department of Computer Science University of Texas at Austinpstone@cs.utexas.edu

# ABSTRACT

최근의 연구는 심층 신경망이 지속적인 상태 및 행동 공간을 특징으로하는 강화 학습 영역에서 가치 기능과 정책을 근사 할 수 있음을 보여주었습니다. 그러나 우리가 아는 한, 구조화 된 \(매개 변수화 된\) 연속 동작 공간에서 심 신경 네트워크를 사용하는 데 이전의 연구가 성공하지 못했습니다. 이러한 차이를 보완하기 위해이 백서에서는 시뮬레이트 된 RoboCup 축구의 영역 내에서 배우는 것에 중점을 둡니다.이 게임에는 연속 변수로 매개 변수화 된 작은 유형의 개별 동작 유형이 있습니다. 최고의 학습자는 2012 RoboCup 챔피언 에이전트보다 더 안정적으로 목표를 기록 할 수 있습니다. 이와 같이,이 논문은 매개 변수화 된 행동 공간 MDP의 클래스에 대한 심층적 인 강화 학습의 성공적인 확장을 나타낸다.

# 1. Introduction

이 논문은 DDPG \(Deep Deterministic Policy Gradients\) 알고리즘 \(Lillicrap et al., 2015\)을 매개 변수화 된 작업 공간으로 확장합니다. 우리는 DDPG 알고리즘의 공개 버전에 대한 수정, 즉 바운딩 액션 공간 구배를 문서화합니다. 우리는이 영역에서 안정적인 학습을 위해 필요한 수정을 발견했으며, 미래의 실무자가 지속적이고 제한적인 행동 공간에서 학습하려고 시도 할 때 유용 할 것입니다.

우리는 골을 넣을 수있는 RoboCup 축구 정책을 처음부터 믿을 수있는 방법으로 학습합니다. 이러한 정책은 낮은 수준의 연속 된 상태 공간과 매개 변수화 된 연속 동작 공간에서 작동합니다. 에이전트는 하나의 보상 기능을 사용하여 공을 찾고 접근하고 목표에 드리블을 부여하며 빈 목표에 점수를 매기는 방법을 학습합니다. 가장 잘 배운 에이전트는 핸드 코딩 된 2012 RoboCup 챔피언보다 느린 속도로 득점 목표에서 더 신뢰할 수 있음을 증명합니다.

RoboCup 2D HFO \(Half-Field-Offense\)는 단일 상담원 학습, 다중 상담원 학습 및 임시 팀 워크를 연구하기위한 연구 플랫폼입니다. HFO는 낮은 수준의 연속적인 상태 공간과 매개 변수화 된 연속 동작 공간을 특징으로합니다. 특히, 매개 변수화 된 작업 공간은 에이전트가 먼저 상위 수준 작업의 개별 목록에서 수행하려는 작업 유형을 선택한 다음 해당 작업을 수반 할 연속 매개 변수를 지정해야합니다. 이 매개 변수화는 순전히 연속적인 동작 공간에서 찾을 수없는 구조를 도입합니다.

이 논문의 나머지 부분은 다음과 같이 구성되어있다. HFO 영역은 2 절에서 제시된다. 3 절에서는 상세한 배우와 비평가 업데이트를 포함한 심층적 인 지속적인 강화 학습에 대한 배경을 제시한다. 5 장은 행동 공간 구배를 제한하는 방법을 제시한다. 6 장에서는 실험과 결과를 다룹니다. 마지막으로, 관련 연구는 8 절에서 결론을 제시한다.



