## Asynchronous Methods for Deep Reinforcement Learning

## 딥 강화학습을 위한 비동기적 방법론

```
Volodymyr Mnih1Adrià Puigdomènech Badia1 Mehdi Mirza1,2Alex Graves1Tim Harley1Timothy P. Lillicrap1David Silver1Koray Kavukcuoglu 11 Google DeepMind2 Montreal Institute for Learning Algorithms (MILA),
University of Montreal
```

#Abstract

우리는 딥 뉴럴 네트워크 컨트롤러의 최적화를 위해 비동기 그래디언트 디센트를 사용하는 심층적인 강화 학습을위한 개념적으로 간단하고 가벼운 프레임 워크를 제안합니다. 우리는 4 개의 표준 강화 학습 알고리즘의 비동기 변형을 제시하고 병렬 배우 학습자가 네 가지 방법 모두 신경 네트워크 컨트롤러를 성공적으로 학습 할 수 있도록 훈련에 안정화 효과가 있음을 보여줍니다. 가장 우수한 수행 방법 인 액터 비평가의 비동기식 변형은 Atari 도메인의 현재 상태를 능가하는 동시에 GPU 대신 단일 멀티 코어 CPU에서 절반의 시간 동안 교육합니다. 또한, 비동기 배우 비평가는 시각적 인 입력을 사용하여 임의의 3D 미로를 탐색하는 새로운 작업뿐만 아니라 다양한 연속 모터 제어 문제에 성공한 것으로 나타났습니다.

#1. Introduction

딥 뉴럴 네트워크는 보강 학습 (RL) 알고리즘을 효과적으로 수행 할 수있는 풍부한 표현을 제공합니다. 그러나 이전에는 간단한 온라인 RL 알고리즘과 딥 뉴럴 네트워크의 결합이 근본적으로 불안정하다고 생각되었습니다. 대신에 알고리즘을 안정화시키기위한 다양한 해법이 제안되었다 (Riedmiller, 2005; Mnih et al., 2013; 2015; Van Hasselt et al., 2015; Schulman et al., 2015a). 이러한 접근법은 공통된 개념을 공유합니다. 온라인 RL 에이전트에 의해 처리되는 관측된 데이터의 시퀀스는 고정적이지 않고 온라인 RL 업데이트는 강하게 상관됩니다. 에이전트의 데이터를 경험 재생 메모리에 저장하여 데이터를 일괄 처리하거나 (Riedmiller, 2005, Schulman et al., 2015a) 또는 무작위로 샘플링 할 수 있습니다 (Mnih et al., 2013, 2015, Van Hasselt 외 2015). 다른 시간 단계. 이러한 방식으로 메모리를 모으는 것은 비 안정성을 줄이고 업데이트를 구체화하지만, 동시에 방법을 강화 학습 알고리즘에 적용하지 못하도록 제한합니다.
경험 리플레이를 기반으로 한 Deep RL 알고리즘은 Atari 2600과 같은 도전적인 영역에서 전례없는 성공을 거두었습니다. 그러나 경험 리플레이는 몇 가지 단점을 가지고 있습니다 : 실제 상호 작용 당 더 많은 메모리와 계산을 사용합니다. 이전 정책에 의해 생성 된 데이터에서 업데이트 할 수있는 오프 정책 학습 알고리즘이 필요합니다.
이 백서에서는 심층 강화 학습을위한 매우 다른 패러다임을 제공합니다. 경험 재생 대신 여러 환경에서 여러 에이전트를 비동기 적으로 동시에 실행합니다. 이 병렬 처리는 에이전트의 데이터를보다 고정 된 프로세스로 분리합니다. 병렬 에이전트가 언제든지 다양한 상태를 경험할 수 있기 때문입니다. 이 간단한 개념은 Sarsa, n-step 방법, actor critic method, Q-learning과 같은 off-policy RL 알고리즘과 같은보다 근본적인 on-policy RL 알고리즘을 강력하게 적용 할 수 있으며 효과적으로 깊은 신경 네트워크를 사용합니다.
우리의 병렬 보강 학습 패러다임은 또한 실질적인 이점을 제공합니다. 이전의 심층 보수 학습은 GPU (Mnih et al., 2015; Van Hasselt et al., 2015; Schaul et al., 2015) 또는 대규모 분산 아키텍처 (Nair 등, 2015), 우리의 실험은 표준 멀티 코어 CPU가 장착 된 단일 시스템에서 실행됩니다. 다양한 Atari 2600 도메인에 적용 할 때 비동기 강화 학습은 이전 GPU 기반 알고리즘보다 훨씬 짧은 시간에 대량 배포 방식보다 훨씬 적은 리소스를 사용하여 더 나은 결과를 얻을 수 있습니다. 제안 된 방법 중 가장 좋은 점은 비동기 우세 행위 비평가 (A3C)는 순수한 시각적 인 입력으로부터 3D 미로를 설명하기위한 일반적인 연속 전략뿐만 아니라 다양한 연속 모터 제어 작업을 마스터했다는 것입니다. 우리는 2D 및 3D 게임, 이산 및 연속 행동 공간뿐 아니라 피드 포워드 및 반복 에이전트를 훈련 할 수있는 능력에 대한 A3C의 성공이 지금까지 가장 보편적이고 성공적인 보강 학습 에이전트가되었다고 생각합니다.

# 2. Related Work

(Nair et al., 2015)의 General Reinforcement Learning Architecture (Gorila)는 분산 설정에서 재 학습 학습 에이전트의 비동기 교육을 수행합니다. GoLilila에서 각 프로세스에는 자체 환경에서 작동하는 액터, 개별 재생 메모리 및 재생 메모리에서 데이터를 샘플링하고 DQN 손실 그라디언트를 계산하는 학습자가 포함되어 있습니다 (Mnih 외 2015) 정책 매개 변수와 관련하여 그라디언트는 모델의 중앙 복사본을 업데이트하는 중앙 매개 변수 서버로 비동기 적으로 전송됩니다. 업데이트 된 정책 매개 변수는 고정 간격으로 액터 학습자에게 전송됩니다. 100 개의 별도 액터 - 학습자 프로세스와 30 개의 매개 변수 서버 인스턴스 (총 130 개의 시스템)를 사용하여 Gorila는 49 개 Atari 게임보다 DQN보다 월등히 뛰어났습니다. 많은 게임에서 Gorila는 DQN보다 20 배 빠른 DQN 점수를 받았습니다. 우리는 DQN을 병렬 처리하는 비슷한 방법이 Chavez et al., 2015에 의해 제안되었다는 것을 또한 주목한다.

이전 연구 (Li & Schuurmans, 2011)에서는 일차 선형 근사법을 사용하여 일괄 강화 학습 방법을 병렬화하기 위해 Map Reduce 프레임 워크를 적용했습니다. 병렬 처리는 대규모 매트릭스 연산의 속도를 높이는 데 사용되었지만 경험 컬렉션을 병렬화하거나 학습을 안정화하는 데 사용되지 않았습니다. (Grounds & Kudenko, 2008)는 훈련을 가속화하기 위해 여러 명의 배우를 사용하는 Sarsa 알고리즘의 병렬 버전을 제안했습니다. 각 배우 - 학습자는 별도로 학습하고 피어 - 투 - 피어 통신을 사용하여 다른 학습자에게 크게 변경된 가중치에 대한 업데이트를 주기적으로 보냅니다.

(Tsitsiklis, 1994)는 비동기 최적화 설정에서 Q- 학습의 수렴 특성을 연구했다. 이 결과는 최신 정보가 항상 폐기되고 다른 여러 가지 기술적 인 가정이 충족되는 경우 정보의 일부가 오래된 경우 Q- 학습이 수렴된다는 것을 보장합니다. 심지어 이전 (Bertsekas, 1982)은 분산 동적 프로그래밍의 관련 문제를 연구했다.

또 다른 관련된 작업 영역은 진화론 적 방법이며, 여러 기계 또는 스레드에 대한 적합성 평가를 분산시킴으로써 병렬화하는 것이 종종 쉽습니다 (Tomassini, 1999). 이러한 평행 진화 과정은 최근 몇 가지 시각 보강 학습 과제에 적용되었다. 한 예 (Koutník et al., 2014)는 병렬로 8 개의 CPU 코어에 대한 적합성 평가를 수행하여 TORCS 주행 시뮬레이터 용 컨볼 루션 뉴럴 네트워크 컨트롤러를 발전 시켰습니다.

# 3. Reinforcement Learning Background

우리는 에이전트가 여러 이산 시간 단계에 걸쳐 환경 E와 상호 작용하는 표준 보강 학습 설정을 고려합니다. 각 시간 단계 t에서 에이전트는 상태 st를 수신하고 정책 π에 따라 가능한 동작 A의 일부 집합에서 동작을 선택합니다. 여기서 π는 상태 st에서 동작으로의 매핑입니다. 그 대가로 에이전트는 다음 상태 st + 1을 수신하고 스칼라 수신 rt를 수신합니다. 프로세스는 프로세스가 다시 시작된 후에 터미널 상태에 도달 할 때까지 계속됩니다. return Rt = ∞k = 0 γkrt + k는 할인 계수 γ ∈ (0, 1)을 갖는 시간 단계 t에서 얻은 총 누적 수익률입니다. 각 에이전트의 기대 수익을 최대화하는 것이 에이전트의 목표입니다.
액션 값 Qπ (s, a) = E [Rt | st = s, a]는 상태 s에서의 액션 a와 정책 π를 선택하기위한 예상 수익입니다. 최적 값 함수 Q * (s, a) = maxπ Qπ (s, a)는 모든 정책에 의해 달성 가능한 상태 s 및 동작 a에 대한 최대 동작 값을 제공합니다. 비슷하게, 정책 π 하에서의 상태 s의 값은 Vπ (s) = E [Rt | st = s]로 정의되며 간단히 상태 s에서의 정책 π에 대한 기대 수익률입니다.
가치 기반 모델없는 보강 학습 방법에서, 동작 값 함수는 신경 회로망과 같은 함수 어플리케이터를 사용하여 표현됩니다. Q (s, a; θ)는 매개 변수 θ를 갖는 근사 작용 값 함수라고합시다. θ에 대한 업데이트는 다양한 강화 학습 알고리즘에서 얻을 수 있습니다. 이러한 알고리즘의 한 가지 예는 최적 동작 값 함수를 직접 근사하는 것을 목표로하는 Q- 학습입니다. Q * (s, a) ≈ Q (s, a; θ). 1 단계 Q- 학습에서, 동작 값 함수 Q (s, a; θ)의 파라미터 θ는 손실 함수의 시퀀스를 반복적으로 최소화함으로써 학습되며, 여기서 i 번째 손실 함수는
 (s, a; θi-1) -Q (s, a; θi)
에이'
여기서 s '는 상태 s 다음에 발생한 상태입니다.
위의 방법을 one-step Q-learning이라고 부르는데, 그 이유는 1-step return r + γ maxa 'Q (s', a '; θ)로 action 값 Q (s, a)를 갱신하기 때문입니다. 원스텝 방법을 사용하는 한 가지 단점은 보상 r을 얻는 것만이 보상으로 이어진 상태 액션 쌍의 값에 직접적으로 영향을 미친다는 것입니다. 다른 상태 액션 쌍의 값은 업데이트 된 값 Q (s, a)를 통해서만 간접적으로 영향을받습니다. 이것은 많은 업 그레 이드가 필요하기 때문에 학습 과정을 느리게 만들 수 있습니다. 이는 관련 선행 상태와 행동에 대한 보상을 전파하는 것입니다.

보상을 빠르게 전파하는 한 가지 방법은 n- 단계 반환을 사용하는 것이다 (Watkins, 1989; Peng & Williams, 1996). n-step Q-learning에서, Q (s, a)는 rt + γrt + 1 +, ... + γn-1rt + n-1 + maxa γnQ (st + n, a ). 이것은 n 개의 선행하는 상태 액션 쌍의 값에 직접적인 영향을 미치는 하나의 보상을 초래합니다. 이는 관련 국가 - 행동 쌍으로 보상을 전파하는 과정을 잠재적으로 훨씬 더 효율적으로 만든다.
정책 기반 모델없는 방법은 가치 기반 방법과는 달리 정책 π (a | s; θ)를 직접 매개 변수화하고 일반적으로 E [Rt]에 대한 대략적인 그라디언트 상승을 수행하여 매개 변수 θ를 업데이트합니다. 그러한 방법의 한 예가 Williams (1992)에 의한 REINFORCE 계열의 알고리즘입니다. 표준 REINFORCE는 정책 파라미터 θ를 방향 ∇θ log π (at | st; θ) Rt로 업데이트하는데, 이는 ∇θE [Rt]의 비 편향 추정입니다. 수익에서 기본 (Williams, 1992)으로 알려진 상태 bt (st)의 학습 된 함수를 뺀 값을 그대로 유지하면서이 추정값의 분산을 줄일 수 있습니다. 결과적인 기울기는 ∇θ log π (at | st; θ) (Rt-bt (st))입니다.
가치 함수의 학습 된 추정치는 일반적으로 정책 기울기의 분산 추정치가 훨씬 낮아지는 기준선 bt (st) ≈ V π (st)로 사용됩니다. 근사값 함수가 기준선으로 사용될 때, 정책 기울기를 스케일하는 데 사용되는 양 Rt-bt는 상태 st에서의 작용 이점의 추정치 또는 A (at, st) = Q ( at, st) -V (st) 왜냐하면 Rt는 Qπ (at, st)의 추정치이고 istestimate는 Vπ (st)의 추정치이기 때문입니다. 이 접근은 정책 π가 행위자이고베이스 라인 bt가 비판 인 행위자 - 비평가 아키텍처로 볼 수있다 (Sutton & Barto, 1998; Degris et al., 2012).

# 4. Asynchronous RL Framework

이제 우리는 1 단계 Sarsa, 1 단계 Q-Learning, n-step Q-Learning 및 장점 배우 평론의 멀티 스레드 비동기 변형을 제시합니다. 이러한 방법을 설계 할 때 목표는 대규모 네트워크 요구 사항없이 안정적으로 깊은 신경 네트워크 정책을 학습 할 수있는 RL 알고리즘을 찾는 것입니다. 근본적인 RL 방법은 상당히 다르지만, 행위자 비평가가 정책에 의한 정책 검색 방법이고 Q 학습이 오프 정책 기반의 방법이기 때문에 우리는 두 가지 주요 아이디어를 사용하여 디자인 목표.
첫째, Gorila 프레임 워크 (Nair et al., 2015)와 마찬가지로 비동기 배우 학습자를 사용하지만 별도의 시스템과 매개 변수 서버를 사용하는 대신 하나의 시스템에서 여러 CPU 스레드를 사용합니다. 학습자를 하나의 컴퓨터에 유지하면 그라디언트와 매개 변수를 전송하는 데 드는 통신 비용이 제거되고 Hogwild! (Recht et al., 2011) 스타일 업데이트를 교육에 적용합니다.
둘째, 우리는 병렬로 실행되는 여러 배우 - 학습자가 환경의 다른 부분을 탐구 할 가능성이 있음을 관찰합니다. 더욱이,이 다양성을 극대화하기 위해 각 배우 - 학습자마다 다른 탐사 정책을 사용하는 것이 명백 할 수 있습니다. 서로 다른 스레드에서 서로 다른 탐색 정책을 실행함으로써 온라인 업데이트를 병렬로 적용하는 여러 액터 - 학습자에 의한 매개 변수의 전반적인 변경 사항은 단일 에이전트가 온라인 업데이트를 적용하는 것보다 시간적으로 덜 일치 할 수 있습니다 . 따라서 우리는 리플레이 메모리를 사용하지 않고 DQN 트레이닝 알고리즘의 경험 재생에 의한 안정화 역할을 수행하기 위해 다양한 탐사 정책을 사용하는 병렬 액터에 의존합니다.
학습을 안정화시키는 것 외에도 여러 개의 평행 배우 학습자를 사용하면 여러 가지 실질적인 이점을 얻을 수 있습니다. 첫째, 우리는 평행 배우 학습자의 수에 거의 선형 인 교육 시간의 단축을 알 수 있습니다. 둘째, 학습을 안정화하기위한 경험 재생에 더 이상 의존하지 않기 때문에 Sarsa 및 배우 평론가와 같은 정책 강화 학습 방법을 사용하여 신경망을 안정적으로 학습 할 수 있습니다. 우리는 이제 1 단계 Q- 학습, 1 단계 Sarsa, n 단계 Q- 학습 및 장점 배우 평론의 변형을 설명합니다.
비동기식 1 단계 Q-learning : 우리가 비동기식 1 단계 Q-learning이라고 부르는 Q-learning의 변종에 대한 의사 코드는 알고리즘 1에 나와 있습니다. 각 스레드는 환경 사본과 각 단계에서 상호 작용합니다 Q- 학습 손실의 기울기를 계산합니다. 우리는 DQN 훈련 방법에서 제안 된 것처럼 Q- 학습 손실을 계산할 때 공유되고 천천히 변화하는 목표 네트워크를 사용합니다. 또한 적용되기 전에 여러 타임 스텝에서 그라디언트를 축적합니다. 이는 미니 버스 사용과 유사합니다. 이렇게하면 여러 명의 학습자가 서로의 업데이트를 덮어 쓸 가능성이 줄어 듭니다. 여러 단계에 걸쳐 업데이트를 업데이트하면 데이터 효율성을 위해 계산 효율성을 상쇄 할 수있는 몇 가지 기능도 제공됩니다.
마지막으로 각 스레드에 다른 탐색 정책을 제공하면 견고성이 향상된다는 것을 알았습니다. 이러한 방식으로 탐사에 다양성을 추가하는 것은 일반적으로 더 나은 탐사를 통해 성과를 향상시킵니다. 탐사 정책을 다양하게 만드는 여러 가지 방법이 있지만, 각 스레드에 의한 분포로부터 주기적으로 샘플링 된 ε- 탐욕 탐사를 사용하여 실험합니다.
비동기식 1 단계 Sarsa : 비동기식 1 단계 Sarsa 알고리즘은 Q (s, a)에 대해 다른 목표 값을 사용한다는 점을 제외하면 알고리즘 1에 주어진 비동기식 1 단계 Q-learning과 동일합니다. 한 단계 Sarsa에 의해 사용 된 목표 값은 a '가 상태 s에서 취한 행동 인 r + γQ (s', a '; θ-)이다 (Rummery & Niranjan, 1994; Sutton & Barto, 1998). 학습을 안정화하기 위해 여러 타임 스텝에 걸쳐 축적 된 목표 네트워크와 업데이트를 다시 사용합니다.
비동기 n-step Q-learning : 다단계 Q-learning의 유사 알고리즘은 Supplementary Algorithm S2에 나와 있습니다. 이 알고리즘은 자격 추적과 같은 기법 (Sutton & Barto, 1998)에서 사용되는보다 일반적인 역방향 뷰와는 달리 n- 단계 리턴을 명시 적으로 계산하여 전방보기에서 작동하기 때문에 약간 이상합니다. 우리는 운동량 기반 방법과 시간에 따른 역 전파를 통해 신경망을 훈련 할 때 전방보기를 사용하는 것이 더 쉽다는 것을 발견했습니다. 단일 업데이트를 계산하기 위해 알고리즘은 먼저 tmax 단계까지 또는 터미널 상태에 도달 할 때까지 탐색 정책을 사용하여 작업을 선택합니다. 이 프로세스를 통해 에이전트는 마지막 업데이트 이후 환경으로부터 tmax 보상까지받을 수 있습니다. 그런 다음 알고리즘은 마지막 업데이트 이후 발생한 각 상태 - 액션 쌍에 대한 n 단계 Q- 학습 업데이트에 대한 그래디언트를 계산합니다. 각 n 단계 업데이트는 가장 긴 가능한 n 단계 반환을 사용하여 마지막 상태에 대한 한 단계 업데이트, 두 번째 마지막 상태에 대한 두 단계 업데이트 등을 수행하여 총 최대 tmax까지 업데이트합니다. 누적 업데이트는 단일 그라데이션 단계에 적용됩니다.

비동기 이점 actor-critic : 알고리즘,
우리는이를 비동기 우대 비평가 (A3C)라고 부릅니다.
정책 π (at | st; θ) 및 값의 추정치를 유지한다
함수 V (st; θv). n-step Q-learning의 변형과 마찬가지로,
배우 비평가의 변종은 또한 전방보기에서 작동합니다.
n 단계 반환의 동일한 혼합을 사용하여
정책 및 가치 기능. 정책과 가치
함수는 모든 tmax 동작 후에 또는
터미널 상태에 도달했습니다. 알 -
gorithm은 ∇θ 'log π (at | st; θ') A (st, at, θ, θv)로 볼 수있습니다.
여기서 A (st, at, θ, θv)는 장점 func-
k-1γir + γkV (s; θ) -V (s; θ), i = 0t + i t + kv tv
여기서 k는 주마다 다를 수 있으며 tmax만큼 상한합니다. 알고리즘의 의사 코드는 보충 알고리즘 S3에 나와 있습니다.
가치 기반 방식과 마찬가지로 우리는 평행 배우 학습자와 훈련 안정성 향상을위한 축적 된 업데이트에 의존합니다. 정책의 매개 변수 θ와 가치 함수의 θv가 일반성을 위해 분리되어있는 것처럼 보여 지지만 실제로 우리는 실제로 일부 매개 변수를 공유합니다. 우리는 전형적으로 정책 π (at | st; θ)에 대해 하나의 softmax 출력을 갖고 값 함수 V (st; θv)에 대해 하나의 선형 출력을 가지며 모든 비 출력 레이어를 공유하는 길쌈 신경망을 사용합니다.
우리는 또한 정책 π의 엔트로피를 목적 함수에 더하면 조기 수렴을 부 최적의 결정 론적 정책으로 낙담시킴으로써 탐사를 개선한다는 것을 발견했다. 이 기술은 원래 계층 적 행동이 필요한 작업에 도움이된다는 것을 발견 한 Williams & Peng (1991)에 의해 처음 제안되었습니다. 정책 파라메터에 대한 엔트로피 정규화 기간을 포함하는 완전한 목적 함수의 기울기는 다음과 같은 형태를 취한다. ∇θ 'log π (at | st; θ') (Rt-V (st; θv)) + β ∇θ 'H (π (st; θ')), 여기서 H는 엔트로피입니다. 과 항 매개 변수 β는 엔트로피 정규화 항의 강도를 제어합니다.
최적화 : 비동기 프레임 워크에서 세 가지 다른 최적화 알고리즘, 즉 모멘텀이있는 SGD, 공유 통계가없는 RMSProp (Tieleman & Hinton, 2012) 및 공유 통계가있는 RMSProp을 조사했습니다. 우리는 표준 non-centered RMSProp 업데이트를 사용했습니다.
Δθg = αg + (1-α) Δθ2 및 θ ← θ-η√g + ε, (1)
여기서 모든 연산은 요소 단위로 수행됩니다. Atari 2600 게임의 하위 집합을 비교하면 통계 g가 스레드에서 공유되는 RMSProp의 변형이 다른 두 방법보다 훨씬 강력하다는 것을 알 수 있습니다. 방법과 비교에 대한 자세한 내용은 보충 섹션 7에 포함되어 있습니다.

# 5. Experiments

우리는 제안 된 프레임 워크의 속성을 평가하기 위해 네 가지 플랫폼을 사용합니다. 우리는 Atari 2600 게임을위한 시뮬레이터를 제공하는 Arcade Learning Environment (Bellemare et al., 2012)를 사용하여 대부분의 실험을 수행합니다. 이것은 RL 알고리즘에서 가장 일반적으로 사용되는 벤치 마크 환경 중 하나입니다. 우리는 Atari 도메인을 사용하여 최첨단 결과와 비교합니다 (Van Hasselt 외 2015, Wang 외 2015, Schaul 외 2015, Nair 외 2015, Mnih 외 2015) 제안 된 방법에 대한 자세한 안정성 및 확장 성 분석을 수행 할 수 있습니다. 우리는 TORCS 3D 카 레이싱 시뮬레이터 (Wymann et al., 2013)를 사용하여 추가 비교를 수행했습니다. 우리는 또한 Mujoco와 Labyrinth라는 A3C 알고리즘만을 평가하기 위해 두 개의 추가 도메인을 사용합니다. MuJoCo (Todorov, 2015)는 접촉 역학을 이용하여 연속적인 모터 제어 작업을 수행하는 에이전트를 평가하는 물리 시뮬레이터입니다. Labyrinth는 에이전트가 시각적 입력에서 무작위로 생성 된 미로에서 보상을 찾는 법을 배워야하는 새로운 3D 환경입니다. 실험 설정에 대한 세부 사항은 8 장에서 찾을 수 있습니다.

##5.1. Atari 2600 게임

먼저 Atari 2600 게임의 하위 집합에 결과를 제시하여 새로운 방법의 학습 속도를 보여줍니다. 그림 1은 Nvidia K40 GPU에서 교육 된 DQN 알고리즘의 학습 속도와 5 개의 Atari 2600 게임에서 16 개의 CPU 코어를 사용하여 교육 된 비동기 방식을 비교 한 것입니다. 결과는 우리가 제시 한 네 가지 비동기식 방법이 Atari 도메인에서 신경망 컨트롤러를 성공적으로 교육 할 수 있음을 보여줍니다. 비동기식 방법은 DQN보다 빠른 속도로 학습하는 경향이 있으며 일부 게임에서는 학습 속도가 상당히 빨라지 며 16 개의 CPU 코어만으로는 학습이 가능합니다. 또한, 결과는 n-step 방법이 일부 게임에서 one-step 방법보다 빨리 학습한다는 것을 보여줍니다. 전반적으로, 정책 기반 우대 행위자 - 비평 방식은 세 가지 가치 기반 방법 모두를 능가하는 것으로 나타났습니다.
그런 다음 57 건의 아타리 게임에 대한 비동기식 우수 배우 평론을 평가했습니다. Atari 게임에서의 최첨단 기술과 비교하기 위해 우리는 주로 훈련 및 평가 프로토콜을 따랐습니다 (Van Hasselt 외 2015). 특히 6 개의 Atari 게임 (Beamrider, Breakout, Pong, Q * bert, Seaquest 및 Space Invaders)에서 검색을 사용하여 하이퍼 매개 변수 (학습 속도 및 그라데이션 표준 클리핑의 양)를 조정 한 다음 모든 57 개의 게임에 대해 모든 하이퍼 매개 변수를 수정했습니다. 우리는 Mnih et al., 2015; Nair et al., 2015; Van Hasselt et al., 2015)와 동일한 아키텍처를 가진 피드 포워드 에이전트와 최종 숨겨진 후 추가적인 256 LSTM 세포를 가진 재발 성 매개체를 훈련시켰다. 층. 또한 평가를 위해 최종 네트워크 가중치를 사용하여 결과를 원래 결과 (Bellemare et al., 2012)와 비교할 수있게했습니다. 우리는 16 명의 CPU 코어를 사용하여 4 일 동안 에이전트를 교육 시켰고 다른 에이전트는 Nvidia K40 GPU에서 8-10 일 동안 교육을 받았습니다. 표 1은 현재의 최첨단 기술뿐 아니라 비동기 우대 비평가 (A3C)가 교육 한 에이전트가 얻은 평균 및 중앙 표준화 된 인간 표준화 점수를 보여줍니다. 보충 테이블 S3은 모든 게임의 점수를 보여줍니다. A3C는 최첨단의 평균 점수가 다른 방법의 훈련 시간의 절반으로 57 게임 이상으로 향상되었으며 단 16 개의 CPU 코어와 GPU는 사용하지 않았습니다. 더군다나 훈련 하루 만에 A3C는 Dueling Double DQN의 평균 인간 표준화 점수와 일치하며 Gorila의 평균 인간 표준화 점수에 거의 도달합니다. Double DQN (Van Hasselt et al., 2015)과 Dueling Double DQN (Wang et al., 2015)에 제시된 많은 개선 사항을 1 단계 Q 및 n 단계 Q 방법에 통합 할 수 있습니다 유사한 잠재력 향상과 함께이 연구에서 발표되었다.

## 5.2. TORCS Car Racing Simulator

우리는 또한 TORCS 3D 자동차 경주 게임에서 4 가지 비동기식 방법을 비교했다 (Wymann et al., 2013). TORCS는 Atari 2600 게임보다 사실적인 그래픽을 제공 할뿐만 아니라 에이전트가 제어하는 ​​차량의 역학을 배우도록 요구합니다. 각 단계에서 에이전트은 현재 프레임의 RGB 이미지 형식의 시각적 입력과 에이전트의 현재 위치에서 에이전트의 속도에 비례 한 보상을받습니다. 우리는 보 조 섹션 8에 명시된 Atari 실험에서 사용 된 것과 동일한 신경망 구조를 사용했다. 우리는 4 가지 설정을 사용하여 실험을 수행했다. 즉, 느린 차를 제어하는 ​​에이전트와 상대 로봇을 제어하는 ​​에이전트와 상대방 봇이 있든 없든 빠른 차. 전체 결과는 보충 그림 S6에서 찾을 수 있습니다. A3C는 최고의 성능을 발휘하는 에이전트로서 약 12 ​​시간의 교육 과정에서 네 가지 게임 구성 모두에 대해 인간 테스터가 얻은 점수의 약 75 %에서 90 %에 이릅니다. A3C 에이전트의 학습 된 운전 행동을 보여주는 비디오는 https://youtu.be/0xo1Ldx3L5Q에서 찾을 수 있습니다.

##5.3. MuJoCo Physics Simulator를 이용한 연속 동작 제어

우리는 또한 행동 공간이 연속되는 일련의 작업을 조사했습니다. 특히 우리는 접촉 역학 (contact dynamics)을 가진 일련의 강체 물리 영역을 살펴 보았는데, 여기서 과제는 조작과 위치의 많은 예를 포함한다. 이러한 작업은 Mujoco 물리 엔진을 사용하여 시뮬레이션되었습니다. 우리는 가치 기반 방법과 달리 연속적인 행동으로 쉽게 확장되기 때문에 비동기 어드밴티지 배우 비평 알고리즘만을 평가했다. 물리적 인 상태 또는 픽셀을 입력으로 사용하는 모든 문제에서 Asynchronous Advantage-Critic은 24 시간 미만의 교육 과정에서 좋은 해결책을 찾았으며 일반적으로 몇 시간 내에 해결되었습니다. 에이전트가 배운 성공적인 정책은 https://youtu.be/ Ajjc08-iPx8 비디오에서 볼 수 있습니다. 이 실험에 대한 자세한 내용은 보충 섹션 9에서 찾을 수 있습니다.

##5.4. 미궁

Labyrinth라는 새로운 3D 환경에서 A3C로 추가 실험을 수행했습니다. 우리가 고려한 특정 작업은 에이전트가 무작위로 생성 된 미로에 대한 보상을 찾아내는 것을 포함했습니다. 각 에피소드가 시작될 때 에이전트는 방과 복도로 구성된 새로운 무작위로 생성 된 미로에 배치되었습니다. 각 미로에는 에이전트가 발견 한 두 가지 유형의 오브젝트 인 사과와 포털이 포함되어있었습니다. 사과를 집어 들면 보상이 1 점이되었습니다. 포털에 입장하면 보상이 10 점이되고 그 후에 에이전트가 미로의 새로운 임의 위치에 다시 만들어졌으며 이전에 수집 된 사과가 모두 재생성되었습니다. 새 에피소드가 시작되는 60 초 후 에피소드가 종료되었습니다. 에이전트의 목적은 제한 시간 내에 가능한 한 많은 포인트를 수집하는 것이고 최적의 전략은 먼저 포털을 찾은 다음 각리스 폰 이후에 포털을 반복적으로 방문하는 것입니다. 에이전트가 각 에피소드에서 새로운 미로를 보게되고 무작위 미로를 탐색하기위한 일반적인 전략을 알아야하기 때문에이 작업은 TORCS 운전 도메인보다 훨씬 어렵습니다.
우리는 입력으로 84x84 RGB 이미지만을 사용하여이 작업에 대한 A3C LSTM 에이전트를 교육했습니다. 최종 평균 점수 약 50은 에이전트가 시각적 입력 만 사용하여 임의의 3D maxes를 탐색하기위한 합리적인 전략을 학습했음을 나타냅니다. 이전에 보이지 않는 미로를 나타내는 에이전트 중 하나를 보여주는 비디오는 https : //youtu.be/nMR5mjCFZCw에 포함되어 있습니다.

##5.5. 확장 성 및 데이터 효율성

우리는 평행 배우 학습자의 수에 따라 교육 시간과 데이터 효율성이 어떻게 변하는 지 살펴봄으로써 제안 된 프레임 워크의 효과 성을 분석했다. 병렬로 여러 근로자를 사용하고 공유 모델을 업데이트 할 때, 주어진 작업과 알고리즘에 대해 이상적인 경우에 특정 점수를 달성하기위한 훈련 단계의 수는 다양한 근로자 수와 동일하게 유지 될 것으로 기대할 수 있습니다. 따라서 이점은 시스템이 같은 양의 벽시계 시간에 더 많은 데이터를 소비하고 탐색 가능성이 향상 될 수 있기 때문입니다. 표 2는 7 개의 Atari 게임에서 사용되는 병렬 배우 학습자의 수가 증가함에 따라 달성되는 교육 속도 향상을 보여줍니다. 이러한 결과는 네 가지 방법 모두 여러 작업자 스레드를 사용하여 상당한 속도 향상을 달성했으며, 16 스레드는 적어도 한 단계 빠른 속도 향상을 나타냅니다. 이는 우리가 제안한 프레임 워크가 병렬 작업자의 수와 잘 맞아 자원을 효율적으로 사용한다는 것을 확인시켜줍니다.

다소 놀랍게도 비동기식 1 단계 Q- 학습 및 Sarsa 알고리즘은 순전히 계산적인 이득으로 설명 할 수없는 초고속 속도 향상을 나타냅니다. 우리는 1 단계 방법 (1 단계 Q와 1 단계 Sarsa)이 더 많은 평행 배우 학습자를 사용할 때 특정 점수를 얻기 위해 더 적은 데이터를 요구한다는 것을 관찰합니다. 이는 한 단계 방법의 편향을 줄이기 위해 다중 스레드의 긍정적 인 효과 때문이라고 생각합니다. 이러한 효과는 5 개의 Atari 게임에서 배우 수강생과 교육 방법의 수에 따른 총 교육 프레임 수 대비 평균 점수 플롯을 보여주는 그림 3과, 벽 시계 시간 대비 erage 점수.
5.6. 견고성 및 안정성
마지막으로 4 개의 제안 된 비동기 알고리즘의 안정성과 강건성을 분석했다. 4 가지 알고리즘 각각에 대해 50 가지 학습 속도와 무작위 초기화를 사용하여 5 가지 게임 (브레이크 아웃, 빔 라이더, 퐁, Q * 버트, 스페이스 인베이더)에 대한 모델을 훈련했습니다. 그림 2는 A3C에 대한 결과 점수의 산점도를 보인 반면, 보충 그림 S11은 다른 세 가지 방법에 대한 도표를 보여줍니다. 일반적으로 각 방법과 게임 조합의 학습 속도 범위는 좋은 점수로 이어 지므로 모든 학습 방법이 학습 속도와 무작위 초기화의 선택에 매우 강력 함을 나타냅니다. 좋은 학습률을 가진 지역에서 0 점을 가진 점수가 거의 없다는 사실은이 방법이 안정적이고 학습 후에 붕괴되거나 분기되지 않는다는 것을 나타냅니다.

#6. 결론 및 토의

우리는 4 개의 표준 강화 학습 알고리즘의 비동기 버전을 제시하고 안정적인 방식으로 다양한 도메인에서 신경 네트워크 컨트롤러를 교육 할 수 있음을 보여주었습니다. 우리의 제안 된 프레임 워크에서 보강 학습을 통한 신경 네트워크의 안정적인 훈련은 가치 기반 및 정책 기반 방법, 오프 - 정책 및 온 - 정책 방법, 그리고 이산 및 연속 간계에서 가능하다는 것을 보여준다 . Atari 도메인에서 16 개의 CPU 코어를 사용하여 교육을 받으면 제안 된 비동기 알고리즘이 Nvidia K40 GPU에서 교육 된 DQN보다 빠르게 학습되며 A3C는 교육 시간의 절반으로 최신 기술을 능가합니다.
우리의 주요 발견 중 하나는 병렬 배우 - 학습자를 사용하여 공유 모델을 업데이트하는 것이 우리가 고려한 세 가지 가치 기반 방법의 학습 과정에 안정적인 영향을 미친다는 것입니다. DQN에서이 목적으로 사용 된 경험 재생없이 안정적인 온라인 Q- 학습이 가능하다는 것을 보여 주지만 경험 재생이 유용하지 않다는 것을 의미하지는 않습니다. 경험 재생을 비동기 강화 학습 프레임 워크에 통합하면 이전 데이터를 다시 사용하여 이러한 방법의 데이터 효율성을 크게 향상시킬 수 있습니다. 이는 TORCS와 같은 도메인에서 훨씬 빠른 교육 시간으로 이어질 수 있습니다. TORCS는 환경과의 상호 작용이 우리가 사용한 아키텍처의 모델을 업데이트하는 것보다 비용이 많이 듭니다.
기존의 다른 보강 학습 방법 또는 심층 강화 학습의 최근 발전과 비동기 프레임 워크를 결합하면 앞서 제시 한 방법을 즉각적으로 개선 할 수있는 많은 가능성을 제시합니다. 우리의 n 단계 방법은 표적으로 수정 된 n 단계 답장을 직접 사용하여 전방보기 (Sutton & Barto, 1998)에서 작동하지만 역 추적보기를 사용하여 적중 추적을 통해 서로 다른 수익을 암시 적으로 결합합니다 ( Watkins, 1989; Sutton & Barto, 1998; Peng & Williams, 1996). 비동기 어드바이저 행위자 - 비평 방법은 잠재적으로 개선 될 수있습니다. Schulman et al. (2015b)의 일반화 된 이점 추정과 같은 이점 함수를 추정하는 다른 방법을 사용하면 잠재적으로 개선 될 수있습니다. 우리가 조사한 모든 가치 기반 방법은 Q 값의 과대 추정 편차를 줄이는 다양한 방법으로 이익을 얻을 수있다 (Van Hasselt et al., 2015; Bellemare et al., 2016). 또 다른보다 투기적인 방향은 진정한 온라인 일시적 차이 방법 (van Seijen et al., 2015)에 대한 최근 연구를 비선형 함수 근사법과 결합하여 시도하는 것입니다.

이러한 알고리즘 개선 외에도 신경망 아키텍처에 대한 보완적인 개선이 가능합니다. 결투 아키텍처 (Wang et al., 2015)는 네트워크에서 국가 가치와 이점에 대한 별도의 스트림을 포함시킴으로써 Q- 값의보다 정확한 시간을 산출하는 것으로 나타났습니다. (Levine et al., 2015)에 의해 제안 된 공간 소프트 - 맥스 (spatial soft-max)는 네트워크가 피쳐 좌표를 표현하기 쉽게함으로써 가치 기반 및 정책 기반 방법을 향상시킬 수있습니다.
감사의 말
논문에 대한 많은 도움이되는 토론, 제안 및 의견을 주신 Thomas Degris, Remi Munos, Marc Lanctot, Sasha Vezhnevets 및 Joseph Modayil에게 감사드립니다. 우리는 또한 DeepMind 평가 팀이이 논문의 에이전트를 평가하는 데 사용 된 환경을 설정해 준 것에 대해 감사드립니다.