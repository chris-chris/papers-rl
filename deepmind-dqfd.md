# DQFD

## Abstract

심층 강화 학습 (RL)은 어려운 의사 결정 문제에서 몇 가지 중요한 성공을 거두었습니다. 그러나 이러한 알고리즘은 일반적으로 합리적인 성능에 도달하기 전에 엄청난 양의 데이터가 필요합니다. 사실, 학습 중 성능은 극도로 떨어질 수 있습니다. 이는 시뮬레이터에서는 수용 가능하지만 에이전트가 실제 환경에서 학습해야하는 많은 실제 작업에 대한 심층 RL의 적용 가능성을 심각하게 제한합니다. 이 논문에서는 에이전트가 시스템의 이전 제어에서 데이터에 액세스 할 수있는 설정을 연구합니다. 우리는 적은 양의 데모 데이터를 활용하여 비교적 적은 양의 데모 데이터로도 학습 프로세스를 대폭 가속화하고 데모 데이터의 필요한 비율을 자동으로 평가할 수있는 데모 (DQfD)의 딥 Q- 학습 알고리즘을 제시합니다 우선 순위가 매겨진 재생 메커니즘 덕분에 학습을 할 수 있습니다. DQfD는 시간차 보정과 악마의 행동에 대한 감독 분류를 결합하여 작동합니다. 우리는 DQfD가 42 가지 게임 중 첫 번째 단계에서 더 좋은 점수로 시작하고 평균적으로 PDD DQN을 포착하기 위해 8300 만 걸음 걸리는 DQfD가 Prioritized Dueling Double Deep Q-Networks (PDD DQN)보다 더 나은 초기 성과를 보임을 보여줍니다 최대 DQfD 성능. DQfD는 42 개 게임 중 14 개 게임에서 제공되는 최고의 데모를 능가하는 것을 배웁니다. 또한 DQfD는 11 가지 게임에 대한 최첨단 결과를 얻기 위해 인간의 플레이 Demonstration를 활용합니다. 마지막으로 DQfD가 데모 데이터를 DQN에 통합하기 위한 세 가지 관련 알고리즘보다 우수함을 보여줍니다.

Deep reinforcement learning (RL) has achieved several high profile successes in difficult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the appli- cability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous con- trol of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning pro- cess even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal dif- ference updates with supervised classification of the demon- strator’s actions. We show that DQfD has better initial per- formance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the first million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD’s performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.

## Introduction

지난 몇 년 동안, 순차적 인 의사 결정 문제와 통제를위한 학습 정책에서 수많은 성공이있었습니다. 주목할만한 예로는 일반적인 Atari 게임 (Mnih 외 2015)을위한 심층 모델없는 Q-learning, 로봇 모터 제어에 대한 종단 간 정책 검색 (Levineetal.2016), 포함 된 모델 예측 제어 ( Watter et al 2015), 검색과 결합 된 전략적 정책은 Go 게임에서 최고의 인간 전문가를 물리 쳤다 (Silver et al., 2016). 이러한 접근 방법의 성공에 중요한 부분은 최근 학습 내용을 확장 학습 및 심화 학습의 성과에 활용하는 것이 었습니다 (LeCun, Bengio, Hinton 2015). 도입 된 접근법 (Mnih et al., 2015)은 배치 RL을 사용하여이 데이터로부터 감독 방식으로 큰 길쌈 신경 네트워크를 훈련시키는 이전 경험 데이터 세트를 구축합니다. 현재의 경험보다는이 데이터 세트로부터 샘플링함으로써, 주 분배 편향에서 얻은 가치의 상관 관계가 완화되어 좋은 (많은 경우 슈퍼 로봇) 제어 정책을 유도합니다.

이러한 알고리즘을 데이터 센터, 자동 차량 (Hester and Stone 2013), 헬리콥터 (Abbeel 외 2007) 또는 추천 시스템 (Shani, Heckerman 및 Brafman 2005). 일반적으로 이러한 알고리즘은 시뮬레이션에서 매우 열악한 성능의 수백만 단계를 거쳐야 만 좋은 제어 정책을 학습합니다. 이 상황은 완벽하게 정확한 시뮬레이터가있는 경우 허용됩니다. 그러나 많은 실제 문제는 그러한 시뮬레이터와 함께 제공되지 않습니다. 대신, 이러한 상황에서 상담원은 실 제 영역에서 실제 행동에 대한 실제 결과를 학습해야하며, 이는 상담원이 학습 시작부터 양호한 온라인 성능을 요구합니다. 정확한 시뮬레이터를 찾기는 어렵지만 이러한 문제의 대부분은 이전 컨트롤러 (사람 또는 기계)에서 시스템이 작동하는 데이터를 합리적으로 잘 수행합니다. 이 작업에서 우리는이 데모 데이터를 사용하여 에이전트를 사전 학습하여 학습 시작 시점부터 잘 수행 할 수 있고 자체 생성 데이터로 계속해서 개선 할 수 있습니다. 이 프레임 워크에서 학습을 활성화하면 데모 데이터가 일반적이지만 정확한 시뮬레이터가 존재하지 않는 많은 실제 문제에 RL을 적용 할 수있는 가능성이 열립니다.

Over the past few years, there have been a number of successes in learning policies for sequential decision-making problems and control. Notable examples include deep model-free Q-learning for general Atari game-playing (Mnih et al. 2015), end-to-end policy search for control of robot motors (Levineetal.2016), model predictive control with embeddings (Watter et al. 2015), and strategic policies that combined with search led to defeating a top human expert at the game of Go (Silver et al. 2016). An important part of the success of these approaches has been to leverage the recent con- tributions to scalability and performance of deep learn- ing (LeCun, Bengio, and Hinton 2015). The approach taken in (Mnih et al. 2015) builds a data set of previous experience using batch RL to train large convolutional neural networks in a supervised fashion from this data. By sampling from this data set rather than from current experience, the correlation in values from state distribution bias is mitigated, leading to good (in many cases, super-human) control policies.
It still remains difficult to apply these algorithms to real world settings such as data centers, au- tonomous vehicles (Hester and Stone 2013), heli- copters (Abbeel et al. 2007), or recommendation sys- tems (Shani, Heckerman, and Brafman 2005). Typically these algorithms learn good control policies only after many millions of steps of very poor performance in simulation. This situation is acceptable when there is a perfectly accu- rate simulator; however, many real world problems do not come with such a simulator. Instead, in these situations, the agent must learn in the real domain with real consequences for its actions, which requires that the agent have good on- line performance from the start of learning. While accurate simulators are difficult to find, most of these problems have data of the system operating under a previous controller (either human or machine) that performs reasonably well. In this work, we make use of this demonstration data to pre-train the agent so that it can perform well in the task from the start of learning, and then continue improving from its own self-generated data. Enabling learning in this framework opens up the possibility of applying RL to many real world problems where demonstration data is common but accurate simulators do not exist.

우리는 매우 적은 양의 데모 데이터를 활용하여 학습 속도를 대폭 향상시키는 DQfD (Deep Q-learning with Demonstrations)라는 새로운 심층 강화 학습 알고리즘을 제안합니다. DQfD는 처음에는 Time-Diferrence (TD)와 Supervised Loss의 조합을 사용하여 데모 데이터에 우선적으로 Pre-train합니다. Supervised Loss는 알고리즘이 Demonstrator를 모방하는 것을 배울 수있게하며, TD 손실은 RL로 학습을 계속할 수있는 자기 일관성 값 기능을 학습 할 수있게합니다. 사전 교육 후에 에이전트는 학습 된 정책으로 도메인과 상호 작용하기 시작합니다. 에이전트는 데모와 자체 생성 데이터가 혼합되어 네트워크를 업데이트합니다. 실제로 데모와자가 생성 데이터 간의 비율을 선택하는 것은 알고리즘의 성능을 향상시키는 데 중요합니다. 우리의 공헌 중 하나는이 비율을 자동으로 제어하기 위해 우선 순위가 매겨진 재생 메커니즘 (Schaul 외 2016)을 사용하는 것입니다. DQfD는 첫 번째 단계 인 42 개 게임 중 41 개 게임에서 Priorityized Dueling Double DQN (PDD DQN) (Schaul 외 2016, van Hasselt, Guez 및 Silver 2016, Wang 외 2016)을 사용하여 순수한 보강 학습을 능가하지 못합니다 , 평균적으로 PDD DQN이 DQfD를 따라 잡기까지 8,300 만 걸음 걸립니다. 또한 DQfD는 42 게임 중 39 게임의 평균 점수에서 순수 모방 학습을 능가하며 42 게임 중 14 게임에서 가장 좋은 데모를 능가합니다. DQfD는 인간 시위를 활용하여 42 개 게임 중 11 개에 최첨단 정책을 학습합니다. 마지막으로 DQfD가 데모 데이터를 DQN에 통합하기위한 세 가지 관련 알고리즘보다 우수한 성능을 보여줍니다.

We propose a new deep reinforcement learning algo- rithm, Deep Q-learning from Demonstrations (DQfD), which leverages even very small amounts of demonstration data to massively accelerate learning. DQfD initially pre- trains solely on the demonstration data using a combination of temporal difference (TD) and supervised losses. The supervised loss enables the algorithm to learn to imitate the demonstrator while the TD loss enables it to learn a self- consistent value function from which it can continue learn- ing with RL. After pre-training, the agent starts interacting with the domain with its learned policy. The agent updates its network with a mix of demonstration and self-generated data. In practice, choosing the ratio between demonstration and self-generated data while learning is critical to improve the performance of the algorithm. One of our contributions is to use a prioritized replay mechanism (Schaul et al. 2016) to automatically control this ratio. DQfD out-performs pure reinforcement learning using Prioritized Duel- ing Double DQN (PDD DQN) (Schaul et al. 2016; van Hasselt, Guez, and Silver 2016; Wang et al. 2016) in 41 of 42 games on the first million steps, and on average it takes 83 million steps for PDD DQN to catch up to DQfD. In addition, DQfD out-performs pure imitation learning in mean score on 39 of 42 games and out-performs the best demonstration given in 14 of 42 games. DQfD leverages the human demonstrations to learn state-of-the-art policies on 11 of 42 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.

## Background

우리는이 연구를 위해 표준 Markov Decision Process (MDP) 법칙을 채택했다 (Sutton and Barto 1998). MDP는 상태 S의 집합, 행동 A의 집합, 보상 함수 R (s, a), 전이 함수 T (s, s)로 구성된 튜플 ⟨S, A, R, T, , a, s ') = P (s'| s, a), 그리고 할인 요인 γ. 에이전트는 각각의 상태 s ∈ S에서 ∈ A의 액션을 취한다.이 행동을 취하면 에이전트는 보상 R (s, a)를 받고 확률 분포 P (s '| s, a). 정책 π는 에이전트가 취할 각 상태에 대해 지정합니다. 에이전트의 목표는 에이전트의 수명 동안 예상되는 할인 된 총 보상을 최대화하는 동작으로 정책 π 매핑 상태를 찾는 것입니다. 주어진 상태 - 행위 쌍 (s, a)의 값 Qπ (s, a)는 정책 π에 뒤따를 때 (s, a)로부터 얻을 수있는 기대 미래 보상의 추정치이다. 최적 값 함수 Q * (s, a)는 모든 상태에서 최대 값을 제공하며 Bellman 방정식을 풀면 다음과 같이 결정됩니다.
그러므로 최적 정책 π는 π (s) = argmaxa∈A Q * (s, a)이다. DQN (Mnih et al., 2015)은 주어진 상태 입력 s에 대한 일련의 활동 값 Q (s, ·; θ)를 출력하는 심 신경 네트워크를 이용하여 값 함수 Q (s, a)를 근사화한다. 네트워크 매개 변수 이 작업을 수행하는 DQN에는 두 가지 주요 구성 요소가 있습니다. 첫째, 목표 Q 값이보다 안정적 이도록 정규 네트워크에서 τ 스텝마다 복사되는 별도의 목표 네트워크를 사용합니다. 둘째, 에이전트는 모든 경험을 재생 버퍼 Dreplay에 추가 한 다음 네트워크에서 업데이트를 수행하기 위해 균일하게 샘플링합니다.
이중 Q-learning 업데이트 (Hasselt, Guez 및 Silver 2016)는 현재 네트워크를 사용하여 다음 상태 값에 대한 argmax와 해당 작업 값에 대한 대상 네트워크를 계산합니다. 이중 DQN 손실은 다음과 같이 나타낼 수있다. 여기서, θ '는 매개 변수이고, Q (s, a, θ) target network의 amax = argmax Q (s, a; θ)이다. 이 두 변수에 사용 된 값 t + 1을 t + 1 함수로 분리하면 일반적인 Q-learning 업데이트로 생성되는 상향 바이어스가 감소합니다. 우선 순위 경험 재생 (Schaul 외 2016)은 DQN 에이전트를 수정하여 재생 버퍼에서 더 중요한 전환을 더 자주 샘플링합니다. 특정 전이 i를 표본 추출 할 확률은 우선 순위 P (i) = i α에 비례하며, 우선 순위 pi = | δi | + | δi는 k pk이다. Q * (s, a) = ER (s, a) + γ P (s '| s, a) max Q * (s', a ')이다.
이 전이에 대해 계산 된 마지막 TD 오차 및 ǫ는 모든 전이가 일부 확률로 샘플링되도록하는 작은 양의 상수이다. 분포의 변화를 고려하기 위해 네트워크 업데이트는 중요도 샘플링 가중치 w = (1 · 1) β로 가중치가 부여됩니다. 여기서 NP는 재생 버퍼이며 β는 중요도가없는 중요도 샘플링 양을 제어합니다 β = 0 인 경우 샘플링하고 β = 1 인 경우 전체 중요도 샘플링을 수행합니다. β는 β0에서 1까지 선형으로 어닐링됩니다.

We adopt the standard Markov Decision Process (MDP) for- malism for this work (Sutton and Barto 1998). An MDP is defined by a tuple ⟨S,A,R,T,γ⟩, which consists of a set of states S, a set of actions A, a reward function R(s, a), a transition function T(s,a,s′) = P(s′|s,a), and a discount factor γ. In each state s ∈ S, the agent takes an action a ∈ A. Upon taking this action, the agent receives a reward R(s, a) and reaches a new state s′, determined from the probability distribution P(s′|s,a). A policy π specifies for each state which action the agent will take. The goal of the agent is to find the policy π mapping states to actions that maximizes the expected discounted total reward over the agent’s life- time. The value Qπ (s, a) of a given state-action pair (s, a) is an estimate of the expected future reward that can be ob- tained from (s,a) when following policy π. The optimal value function Q∗(s, a) provides maximal values in all states and is determined by solving the Bellman equation:

The optimal policy π is then π(s) = argmaxa∈A Q∗(s, a). DQN (Mnih et al. 2015) approximates the value function Q(s, a) with a deep neural network that outputs a set of ac- tion values Q(s, ·; θ) for a given state input s, where θ are the parameters of the network. There are two key components of DQN that make this work. First, it uses a separate target net- work that is copied every τ steps from the regular network so that the target Q-values are more stable. Second, the agent adds all of its experiences to a replay buffer Dreplay , which is then sampled uniformly to perform updates on the net- work.

The double Q-learning up-date (van Hasselt, Guez, and Silver 2016) uses the current network to calculate the argmax over next state values and the target network for the value of that action. The double DQN loss is J (Q) =  R(s,a)+γQ(s ,amax;θ′)−Q(s,a;θ) 2, DQ t+1 t+1 where θ′ are the parameters of the target network, and amax = argmax Q(s , a; θ). Separating the value t+1 a t+1 functions used for these two variables reduces the upward bias that is created with regular Q-learning updates. Prioritized experience replay (Schaul et al. 2016) modi- fies the DQN agent to sample more important transitions from its replay buffer more frequently. The probability of sampling a particular transition i is proportional to its priority, P(i)=   i α ,wheretheprioritypi =|δi|+ǫ,andδi is k pk Q∗(s, a) = E  R(s, a) + γ   P (s′|s, a) max Q∗(s′, a′). 

the last TD error calculated for this transition and ǫ is a small positive constant to ensure all transitions are sampled with some probability. To account for the change in the distribution, updates to the network are weighted with importance sampling weights, w =(1 · 1 )β,whereN isthesizeof i NP(i) the replay buffer and β controls the amount of importance sampling with no importance sampling when β = 0 and full importance sampling when β = 1. β is annealed linearly from β0 to 1.

모방 학습은 주로 시위자의 성과를 맞추는 것과 관련이 있습니다. 한 가지 인기있는 알고리즘 인 DAGGER (Ross, Gordon 및 Bagnell 2011)는 원래 상태 공간 외부의 전문가 정책을 폴링하여 새로운 정책을 반복적으로 생성하여 온라인 학습 관점에서 유효성 검사 데이터에 대해 후회하지 않습니다. DAGGER는 훈련 중에 상담원에게 추가적인 피드백을 제공 할 수 있도록 전문가의 도움을 필요로합니다. 또한, 모방과 보강 학습을 결합하지 않기 때문에 DQfD가 할 수있는 것처럼 전문가 이상의 수준으로 향상시키는 법을 배울 수는 없습니다.
Deep AggreVaTeD (Sun 외 2017)는 DAGGER를 확장하여 심층 신경 네트워크 및 연속 동작 공간에서 작동합니다. DAGGER처럼 항상 사용할 수있는 전문가가 필요 할뿐만 아니라 전문가는 작업 외에 가치 기능을 제공해야합니다. DAGGER와 마찬가지로 Deeply AgVaTeD는 모방 학습 만하고 전문가를 향상시키는 법을 배울 수 없습니다.
다른 대중적인 패러다임은 학습자가 정책을 선택하고 적의가 보상 기능을 선택하는 제로섬 게임을 설정하는 것입니다 (Syed and Schapire 2007, Syed, Bowling, Schapire 2008, Ho and Ermon 2016). 데모는 또한 고차원적이고 연속적인 로봇 제어 문제 (Finn, Levine 및 Abbeel 2016)에서 역 최적 제어를 위해 사용되었습니다. 그러나 이러한 접근법은 모방 학습을 수행하고 업무 보상에서 학습을 허용하지 않습니다.
최근 RL (Subramanian, Jr. 및 Thomaz 2016)의 어려운 탐사 문제를 돕기위한 데모 데이터가 나와 있습니다. 이 결합 된 모조 및 RL 문제에 대한 최근의 관심이 또한있었습니다. 예를 들어, HAT 알고리즘은 인간 정책 (Taylor, Suay 및 Chernova 2011)에서 직접 지식을 전송합니다. 이 연구의 후속 조치는 RL 문제 (Brys 외 2015; Suay 외 2016)에서 보상을 형성하기 위해 전문가의 조언이나 시연을 어떻게 사용할 수 있는지를 보여 주었다.

Imitation learning is primarily concerned with matching the performance of the demonstrator. One popular algorithm, DAGGER (Ross, Gordon, and Bagnell 2011), iteratively produces new policies based on polling the expert policy outside its original state space, showing that this leads to no-regret over validation data in the online learning sense. DAGGER requires the expert to be available during training to provide additional feedback to the agent. In addition, it does not combine imitation with reinforcement learning, meaning it can never learn to improve beyond the expert as DQfD can.
Deeply AggreVaTeD (Sun et al. 2017) extends DAGGER to work with deep neural networks and continuous action spaces. Not only does it require an always available expert like DAGGER does, the expert must provide a value function in addition to actions. Similar to DAGGER, Deeply Ag- greVaTeD only does imitation learning and cannot learn to improve upon the expert.
Another popular paradigm is to setup a zero-sum game where the learner chooses a policy and the adversary chooses a reward function (Syed and Schapire 2007; Syed, Bowling, and Schapire 2008; Ho and Ermon 2016). Demonstrations have also been used for inverse optimal control in high-dimensional, continuous robotic control problems (Finn, Levine, and Abbeel 2016). However, these approaches only do imitation learning and do not allow for learning from task rewards.
Recently, demonstration data has been shown to help in difficult exploration problems in RL (Subramanian, Jr., and Thomaz 2016). There has also been recent interest in this combined imitation and RL problem. For example, the HAT algorithm transfers knowledge directly from human policies (Taylor, Suay, and Chernova 2011). Follow-ups to this work showed how expert advice or demonstrations can be used to shape rewards in the RL problem (Brys et al. 2015; Suay et al. 2016).

다른 접근법은 경험을 샘플링하는 데 사용되는 정책을 형성하거나 (Cederborg 외 2015), 데모 Chemali 및 Lezaric 2015에서 정책 반복을 사용하는 것입니다.

우리 알고리즘은 시위자가 사용하는 환경에서 보상을받는 시나리오에서 작동합니다. 이 프레임 워크는 적절하게 (Piot, Geist, Pietquin 2014a)의 RLED (Reinforcement Learning with Expert Demonstrations)라고도하며 (Kim et al., 2013, Chemali and Lezaric 2015) 평가됩니다. 우리의 설정은 모형없는 설정에서 배치 알고리즘에서 TD와 분류 손실을 결합한다는 점에서 (Piot, Geist 및 Pietquin 2014a)와 유사합니다. 우리의 에이전트는 초기에 데모 데이터에 대한 사전 교육을 받았으며 자체 생성 데이터 배치가 시간이 지남에 따라 증가하고 깊은 Q 네트워크를 교육하기위한 경험 재생으로 사용된다는 점이 다릅니다. 또한, 각 미니 배치에서 데모 데이터의 양을 균형을 맞추기 위해 우선 순위가 지정된 재생 메커니즘이 사용됩니다. (Piot, Geist 및 Pietquin 2014b)는 감독 된 분류 손실에 TD 손실을 추가하면 보상이없는 경우에도 모방 학습이 향상된다는 흥미로운 결과가 나타납니다.

A different approach is to shape the policy that is used to sample experience (Cederborg et al. 2015), or to use policy iteration from demonstrations Chemali and Lezaric 2015).

Our algorithm works in a scenario where rewards are given by the environment used by the demonstrator. This framework was appropriately called Rein- forcement Learning with Expert Demonstrations (RLED) in (Piot, Geist, and Pietquin 2014a) and is also evaluated in (Kim et al. 2013; Chemali and Lezaric 2015). Our setup is similar to (Piot, Geist, and Pietquin 2014a) in that we combine TD and classification losses in a batch algo- rithm in a model-free setting; ours differs in that our agent is pre-trained on the demonstration data initially and the batch of self-generated data grows over time and is used as experience replay to train deep Q-networks. In addition, a prioritized replay mechanism is used to bal- ance the amount of demonstration data in each mini-batch. (Piot, Geist, and Pietquin 2014b) present interesting results showing that adding a TD loss to the supervised classifica- tion loss improves imitation learning even when there are no rewards.


Another work that is similarly motivated to ours is (Schaal 1996). This work is focused on real world learning on robots, and thus is also concerned with on-line perfor- mance. Similar to our work, they pre-train the agent with demonstration data before letting it interact with the task. However, they do not use supervised learning to pre-train their algorithm, and are only able to find one case where pre-training helps learning on Cart-Pole.
In one-shot imitation learning (Duanetal.2017), the agent is provided with an entire demonstration as input in addition to the current state. The demonstration specifies the goal state that is wanted, but from different initial conditions. The agent is trained with target actions from more demon- strations. This setup also uses demonstrations, but requires a distribution of tasks with different initial conditions and goal states, and the agent can never learn to improve upon the demonstrations.
AlphaGo (Silver et al. 2016) takes a similar approach to our work in pre-training from demonstration data before in- teracting with the real task. AlphaGo first trains a policy network from a dataset of 30 million expert actions, using supervised learning to predict the actions taken by experts. It then uses this as a starting point to apply policy gradient updates during self-play, combined with planning rollouts. Here, we do not have a model available for planning, so we focus on the model-free Q-learning case.
Human Experience Replay (HER) (Hosu and Rebedea 2016) is an algorithm in which the agent samples from a replay buffer that is mixed between agent and demonstration data, similar to our approach. Gains were only slightly better than a random agent, and were surpassed by their alternative approach, Human Checkpoint Replay, which requires the ability to set the state of the environment. While their algorithm is similar in that it samples from both datasets, it does not pre-train the agent or use a supervised loss. Our results show higher scores over a larger variety of games, without requiring full access to the environment. Replay Buffer Spiking (RBS) (Lipton et al. 2016) is another similar approach where the DQN agent’s replay buffer is initialized with demonstration data, but they do not pre-train the agent for good initial performance or keep the demonstration data permanently.
The work that most closely relates to ours is a workshop paper presenting Accelerated DQN with Expert Trajecto- ries (ADET) (Lakshminarayanan, Ozair, and Bengio 2016). They are also combining TD and classification losses in a deep Q-learning setup. They use a trained DQN agent to generate their demonstration data, which on most games is better than human data. It also guarantees that the policy used by the demonstrator can be represented by the appren- ticeship agent as they are both using the same state input and network architecture. They use a cross-entropy classifi- cation loss rather than the large margin loss DQfD uses and they do not pre-train the agent to perform well from its first interactions with the environment.

## Deep Q-Learning from Demonstrations

많은 실제 환경에서 보강 학습의 경우 이전 컨트롤러가 작동하는 시스템의 데이터에 액세스 할 수 있지만 시스템의 정확한 시뮬레이터에 액세스 할 수는 없습니다. 그러므로 에이전트는 실제 시스템을 실행하기 전에 데모 데이터에서 가능한 한 많이 배우기를 원합니다. 사전 교육 단계의 목표는 에이전트가 환경과 상호 작용을 시작하면이 상향 TD 업데이트와 함께 일 할 수 있도록 벨맨 방정식을 만족 func- 기 값과 논증을 모방하는 법을 배워야하는 것입니다. 이 미리 트레이닝 단계 동안, 제 샘플 시연 데이터로부터 미니 일괄와 위쪽 네 손실을 적용하여 네트워크를 기간 : 1 단계 이중 Q 학습 손실의 n 단계 이중 Q 학습 손실하는 슈퍼 - 큰 마진 분류 손실 및 네트워크 가중치 및 바이어스에 대한 L2 정규화 손실을 고려해야합니다. 감시 된 손실은 시연자의 행동 분류에 사용되며, Q- 학습 손실은 네트워크가 Bellman 방정식을 만족시키고 TD 학습을위한 출발점으로 사용될 수 있음을 보장합니다.
감독 된 손실은 사전 훈련이 어떤 영향을 미치기 위해 중요합니다. 데모 데이터는 반드시 국가 공간의 좁은 부분을 다루고 모든 가능한 행동을 취하지 않기 때문에 많은 국가 행동이 결코 취해지지 않았고 현실적인 가치에 근거 할 수있는 어떠한 데이터도 가지고 있지 않다. 우리는 다음 상태의 최대 값으로 만 Q-학습 업데이트로 네트워크를 사전 훈련을한다면, 네트워크는 이러한 접지 변수의 가장으로 업데이트 할 것이며,이 값을 전파 할 네트워크는 Q 기능을 전역 개 . 우리는 큰 마진 분류 손실 (Piot, Geist 및 Pietquin 2014a)을 추가합니다.

In many real-world settings of reinforcement learning, we have access to data of the system being operated by its previ- ous controller, but we do not have access to an accurate sim- ulator of the system. Therefore, we want the agent to learn as much as possible from the demonstration data before run- ning on the real system. The goal of the pre-training phase is to learn to imitate the demonstrator with a value func- tion that satisfies the Bellman equation so that it can be up- dated with TD updates once the agent starts interacting with the environment. During this pre-training phase, the agent samples mini-batches from the demonstration data and up- dates the network by applying four losses: the 1-step double Q-learning loss, an n-step double Q-learning loss, a super- vised large margin classification loss, and an L2 regulariza- tion loss on the network weights and biases. The supervised loss is used for classification of the demonstrator’s actions, while the Q-learning loss ensures that the network satisfies the Bellman equation and can be used as a starting point for TD learning.
The supervised loss is critical for the pre-training to have any effect. Since the demonstration data is necessarily cov- ering a narrow part of the state space and not taking all pos- sible actions, many state-actions have never been taken and have no data to ground them to realistic values. If we were to pre-train the network with only Q-learning updates to- wards the max value of the next state, the network would update towards the highest of these ungrounded variables and the network would propagate these values through- out the Q function. We add a large margin classification loss (Piot, Geist, and Pietquin 2014a):

여기서 aE는 전문가 시위자가 국가에서 취한 행동이고 l (aE, a)는 a = aE 일 때 0이고 그렇지 않으면 양수인 마진 기능이다. 이 손실은 다른 행동의 가치를 적어도 시위자의 행동 가치보다 낮은 마진으로 만든다. 이 손실을 추가하면 보이지 않는 행동의 가치를 합리적인 가치로 끌어 올리며 가치 기능에 의해 유도 된 욕심 많은 정책을 시위자를 모방하게 만듭니다. 알고리즘이이 예비 손실만을 사용하여 사전 훈련 된 경우, 연속 상태와 Q- 네트워크 사이의 값을 제한하는 것은 TD와 함께 온라인 정책을 개선하는 데 필요한 Bellman 방정식을 충족시키지 못합니다 배우기.
n-step return (n = 10)을 추가하면 전문가의 궤도 값을 모든 이전 상태로 전파하여 사전 교육을 향상시킬 수 있습니다. n 단계 반환 값은 다음과 같습니다.

where aE is the action the expert demonstrator took in state s and l(aE,a) is a margin function that is 0 when a = aE and positive otherwise. This loss forces the values of the other actions to be at least a margin lower than the value of the demonstrator’s action. Adding this loss grounds the val- ues of the unseen actions to reasonable values, and makes the greedy policy induced by the value function imitate the demonstrator. If the algorithm pre-trained with only this su- pervised loss, there would be nothing constraining the val- ues between consecutive states and the Q-network would not satisfy the Bellman equation, which is required to improve the policy on-line with TD learning.
Adding n-step returns (with n = 10) helps propagate the values of the expert’s trajectory to all the earlier states, lead- ing to better pre-training. The n-step return is:

$$rt + γrt+1 + ... + γn−1rt+n−1 + maxaγnQ(st+n, a),$$

우리는 A3C (Mnih et al. 2016)와 유사하게 전방보기를 사용하여 계산합니다.
우리는 또한 네트워크의 가중치와 편향에 적용된 L2 정규화 손실을 추가하여 상대적으로 작은 데모 데이터 세트에 과도하게 끼워지는 것을 방지합니다. 네트워크를 업데이트하는 데 사용 된 전체 손실은 네 가지 손실 모두를 합한 것입니다.

which we calculate using the forward view, similar to A3C (Mnih et al. 2016).
We also add an L2 regularization loss applied to the weights and biases of the network to help prevent it from over-fitting on the relatively small demonstration dataset. The overall loss used to update the network is a combina- tion of all four losses:

$$J(Q) = JDQ(Q) + λ1Jn(Q) + λ2JE(Q) + λ3JL2(Q).$$

λ 매개 변수는 손실 사이의 가중치를 제어합니다. 섹션에서 이러한 손실 중 일부를 제거하는 방법을 검토합니다.
사전 교육 단계가 완료되면 에이전트가 시스템에서 작동하여 자체 생성 데이터를 수집하고이를 재생 버퍼 Dreplay에 추가합니다. 데이터가 찰 때까지 재생 버퍼에 데이터가 추가 된 후 에이전트는 해당 버퍼의 이전 데이터를 덮어 쓰기 시작합니다. 그러나 에이전트는 데모 데이터를 덮어 쓰지 않습니다. 비례 사전 표본 추출의 경우 에이전트와 데모 전환의 우선 순위에 다른 작은 양의 상수 ǫa와 ǫd가 추가되어 데모 대 에이전트 데이터의 상대적 샘플링을 제어합니다. 두 단계 모두에서 모든 손실이 악마 데이터에 적용되지만 감독 손실은 자체 생성 데이터 (λ2 = 0)에는 적용되지 않습니다.
전반적으로, DQfD (Deep Q-learning with Demonstration)는 PDD DQN과 6 가지 주요 방법이 다릅니다.

The λ parameters control the weighting between the losses. We examine removing some of these losses in Section .
Once the pre-training phase is complete, the agent starts acting on the system, collecting self-generated data, and adding it to its replay buffer Dreplay. Data is added to the replay buffer until it is full, and then the agent starts over- writing old data in that buffer. However, the agent never over-writes the demonstration data. For proportional prior- itized sampling, different small positive constants, ǫa and ǫd, are added to the priorities of the agent and demonstration transitions to control the relative sampling of demonstration versus agent data. All the losses are applied to the demon- stration data in both phases, while the supervised loss is not applied to self-generated data (λ2 = 0).
Overall, Deep Q-learning from Demonstration (DQfD) differs from PDD DQN in six key ways: