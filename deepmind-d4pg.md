#DISTRIBUTED DISTRIBUTIONAL DETERMINISTIC(D4PG)
POLICY GRADIENTS

Gabriel Barth-Maron˚, Matthew W. Hoffman˚, David Budden, Will Dabney,
Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, Timothy Lillicrap
DeepMind
London, UK

## Abstract 

이 연구는 강화 학습에 대한 매우 성공적인 분배 관점을 채택하고이를 연속 제어 설정에 적용합니다. Distributed Distributed Deep Deterministic Policy Gradient 알고리즘 인 D4PG를 개발하기 위해 오프 정책 학습을위한 분산 프레임 워크 내에서 이것을 결합합니다.
또한이 기술을 N 단계 수익률 및 우선 순위 경험 재생과 같은 여러 가지 추가적이고 간단한 개선과 결합합니다. 실험적으로이 개별 구성 요소 각각의 기여도를 검토하고 상호 작용 방식 및 기여도를 보여줍니다. 우리의 결과는 D4PG 알고리즘이 최첨단 성능을 달성하는 다양한 간단한 제어 작업, 어려운 조작 작업 및 일련의 어려운 장애물 기반의 이동 작업에 걸쳐 있음을 보여줍니다.

This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG.
We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.

## 1 INTRODUCTION

high dimensional input 및 action space으로 복잡한 제어 작업을 해결하는 기능은 핵심 요소입니다
실제 인공 지능 개발에 획기적인 이정표가되었습니다. 이러한 유형의 작업을 해결하기위한 강화 학습의 사용은 많은 Atari 게임에서 인간 수준의 성능을 발휘할 수있는 DeepQ Network (DQN) 알고리즘 (Mnih et al., 2015)의 작업에 따라 폭발적으로 증가했습니다. 비슷하게, Go (Silver et al., 2016)와 같은 고전 게임에서도 근거가 깨어졌습니다. 그러나 이러한 알고리즘은 제한된 수의 개별 동작에 대한 문제로 제한됩니다.
로봇 작업 영역에서 흔히 볼 수있는 제어 작업에서는 연속적인 동작 공간이 일반적입니다.
DQN과 같은 알고리즘의 경우 정책은 값 기능과 관련하여 암시 적으로 정의되며이 기능을 최대화하여 작업을 선택합니다. 연속 제어 영역에서는 비용이 많이 드는 최적화 단계 나 작업 공간의 이산화가 필요합니다. 이산화 (discretization)는 아마도 가장 간단한 해결책 일 수 있지만, 이것은 고차원 설정이나보다 정교한 제어가 필요한 경우에 특히 좋지 않은 근사를 증명할 수 있습니다. 대신 정책을 명시 적으로 매개 변수화하고이 정책을 따르는 장기적인 가치를 직접 최적화하는 것이 더 원칙적인 방법입니다.
이 연구에서는 DDPG (Deep Deterministic Policy Gradient) 알고리즘 (Lillicrap et al., 2015)에 대한 여러 가지 수정 사항을 고려합니다. 이 알고리즘은 우리가 고려해야하는 개선점에 이상적인 몇 가지 속성을 가지고 있습니다. 핵심은 Off-Policy Actor-Critic 방식입니다. 특히, Actor 네트워크를 업데이트하는 데 사용되는 정책 그래디언트는 학습 된 비평가에 의해서만 결정됩니다. 즉, 비평 학습 절차를 개선하면 배우 업데이트의 품질이 직접 향상됩니다. 이 연구에서 우리는 더 나은,보다 안정적인 학습 신호를 제공하는 비판적 업데이트 (Bellemare et al., 2017) 버전을 사용한다. 이러한 분포는 내재적 요인으로 인한 무작위성을 모델링하며, 그 중 연속 환경에서의 함수 근사에 의해 내재 된 고유 한 불확실성이다. 이 배포판 업데이트를 사용하면 더 나은 결과를 얻을 수 있습니다.
따라서 학습 알고리즘의 성능이 향상됩니다.

DDPG가 오프 정책을 배울 수 있다는 사실 때문에 경험이 수집되는 방식을 수정할 수도 있습니다. 이 작업에서 우리는이 사실을 이용하여 많은 액터를 병렬로 실행합니다. 모든 액터는 단일 재생 테이블로 공급됩니다. 이를 통해 우리는 ApeX 프레임 워크 (Horgan et al., 2018)를 사용하여 구현 한 경험 수집 작업을 원활하게 배포 할 수 있습니다. 이로 인해 어려운 제어 작업에 대한 벽시계 시간이 크게 절약됩니다. 우리는 또한 소개 할 것이다.
DDPG 알고리즘에 대한 몇 가지 작은 개선 사항 및 우리 실험에서 각 구성 요소의 개별 기여도를 보여줍니다. 마지막으로 D4PG (Distributed Distributional DDPG) 알고리즘이라고하는이 알고리즘은 하드 조작 및 이동 작업을 포함하여 다양한 제어 작업에서 최첨단 성능을 얻습니다.

The ability to solve complex control tasks with high-dimensional input and action spaces is a key
milestone in developing real-world artificial intelligence. The use of reinforcement learning to solve these types of tasks has exploded following the work of the Deep Q Network (DQN) algorithm (Mnih et al., 2015), capable of human-level performance on many Atari games. Similarly, ground breaking achievements have been made in classical games such as Go (Silver et al., 2016). However, these algorithms are restricted to problems with a finite number of discrete actions.
In control tasks, commonly seen in the robotics domain, continuous action spaces are the norm.
For algorithms such as DQN the policy is only implicitly defined in terms of its value function, with actions selected by maximizing this function. In the continuous control domain this would require either a costly optimization step or discretization of the action space. While discretization is perhaps the most straightforward solution, this can prove a particularly poor approximation in highdimensional settings or those that require finer grained control. Instead, a more principled approach is to parameterize the policy explicitly and directly optimize the long term value of following this policy.
In this work we consider a number of modifications to the Deep Deterministic Policy Gradient (DDPG) algorithm (Lillicrap et al., 2015). This algorithm has several properties that make it ideal for the enhancements we consider, which is at its core an off-policy actor-critic method. In particular, the policy gradient used to update the actor network depends only on a learned critic. This means that any improvements to the critic learning procedure will directly improve the quality of the actor updates. In this work we utilize a distributional (Bellemare et al., 2017) version of the critic update which provides a better, more stable learning signal. Such distributions model the randomness due to intrinsic factors, among these is the inherent uncertainty imposed by function approximation in a continuous environment. We will see that using this distributional update directly results in better
gradients and hence improves the performance of the learning algorithm.

Due to the fact that DDPG is capable of learning off-policy it is also possible to modify the way in which experience is gathered. In this work we utilize this fact to run many actors in parallel, all feeding into a single replay table. This allows us to seamlessly distribute the task of gathering experience, which we implement using the ApeX framework (Horgan et al., 2018). This results in significant savings in terms of wall-clock time for difficult control tasks. We will also introduce
a number of small improvements to the DDPG algorithm, and in our experiments will show the individual contributions of each component. Finally, this algorithm, which we call the Distributed Distributional DDPG algorithm (D4PG), obtains state-of-the-art performance across a wide variety of control tasks, including hard manipulation and locomotion tasks.

## 1.1 RELATED WORK

역사적으로, 정책 구배의 추정은 강화 학습 공동체에서 REINFORCE (Williams, 1992)로 더 일반적으로 알려진 우도 비율 트릭 (예 : Glynn, 1990 참조)에 의존했습니다. 이러한 소위 "바닐라"정책 구배 방법의 현대적 변형은 (Mnih et al., 2016)의 연구를 포함한다. 대안으로,이 목적의 2 차 또는 "자연"변이체, 예를 들어, Natural Actor-Critic (Peters & Schaal, 2008) 및 Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) 알고리즘을 사용합니다. 최근 TRPO의 근사치로 볼 수있는 PPO (Proximal Policy Optimization) (Schulman et al., 2017)는 대규모 분산 설정에서 매우 효과적임이 입증되었습니다. 그러나 종종이 양식의 알고리즘은 정책을 학습하는 것으로 제한되어 있으므로 데이터 재사용 량을 제한하고 탐색에 사용되는 정책 유형을 제한 할 수 있습니다.

이 연구의 기초가되는 결정론 정책 기울기 (Deterministic Policy Gradient, DPG) 알고리즘 (Silver et al., 2014)은 다른 아이디어들, 즉 정책 구배 정리 (Sutton et al., 2000)에서 출발한다. 결정 론적 정책 구배 정리는이 초기 접근법을 기반으로하지만 확률 론적 정책을 무작위성을 포함하지 않는 정책으로 바꿉니다. 이 접근법은 특히 중요합니다.

이전에는 결정적 정책 기울기가 모델이없는 환경에 존재하지 않는다고 생각 되었기 때문입니다. 이 그라데이션의 형태는 액션 공간에 통합 할 필요가 없으므로 배울 수있는 샘플이 더 적을 수도 있다는 점에서 흥미 롭습니다. DPG는 후에 Lillicrap et al. 이 알고리즘을 확장하고 함수 근사자로 깊은 신경망을 사용하는 (2015), 주로 이러한 결과를 확장하여 비전 기반 입력. 게다가,이 모든 노력은 배우의 그라디언트가 학문적 비평가를 통해 파생 상품에만 의존하기 때문에 오프 정책 actorcritic 건축술에 아주 쉽게 빌려 준다. 이것은 비평가의 평가를 향상시킴으로써 배우의 그라디언트를 직접 개선 할 수 있음을 의미합니다. 가장 흥미롭게도, DDPG 알고리즘 (예 : Popov et al., 2017)에 대한 업데이트를 배포하려는 최근의 시도가 있었으며,보다 일반적으로 분산 된 액터 구현을 위해 (Horgan et al., 2018) 작업을 기반으로이 작업을 수행했습니다.

최근 Bellemare et al. (2017)은 기대치가 가치 함수 인, 수익률에 대한 분배가 분포 Bellman 방정식을 따른다는 것을 보여 주었다. 수익률에 대한 분배를 추정하는 아이디어가 이전에 재검토되었지만 (Sobel, 1982; Morimura et al., 2010), Bellemare et al. 이 추정만으로도 Atari 2600 벤치 마크에서 최첨단 결과를 달성하기에 충분하다는 것을 입증했습니다. 결정적으로이 기술은 비평가의 업데이트를 직접 개선하여 이러한 이점을 얻습니다.

Historically, estimation of the policy gradient has relied on the likelihood ratio trick (see e.g. Glynn, 1990), more commonly known as REINFORCE (Williams, 1992) in the reinforcement learning community. Modern variants of these so-called “vanilla” policy gradient methods include the work of (Mnih et al., 2016). Alternatively, one can consider second-order or “natural” variants of this objective, a set of techniques that include e.g. the Natural Actor-Critic (Peters & Schaal, 2008) and Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) algorithms. More recently Proximal Policy Optimization (PPO) (Schulman et al., 2017), which can be seen as an approximation of TRPO, has proven very effective in large-scale distributed settings. Often, however, algorithms of this form are restricted to learning on-policy, which can limit both the amount of data-reuse as well as restrict the types of policies that are used for exploration.
The Deterministic Policy Gradient (DPG) algorithm (Silver et al., 2014) upon which this work is based starts from a different set of ideas, namely the policy gradient theorem of (Sutton et al., 2000). The deterministic policy gradient theorem builds upon this earlier approach, but replaces the stochastic policy with one that includes no randomness. This approach is particularly important
because it had previously been believed that the deterministic policy gradient did not exist in a model-free setting. The form of this gradient is also interesting in that it does not require one to integrate over the action space, and hence may require less samples to learn. DPG was later built upon by Lillicrap et al. (2015) who extended this algorithm and made use of a deep neural network as the function approximator, primarily as a mechanism for extending these results to work with
vision-based inputs. Further, this entire endeavor lends itself very readily to an off-policy actorcritic architecture such that the actor’s gradients depend only on derivatives through the learned critic. This means that by improving estimation of the critic one is directly able to improve the actor gradients. Most interestingly, there have also been recent attempts to distribute updates for the DDPG algorithm, (e.g. Popov et al., 2017) and more generally in this work we build on work of (Horgan et al., 2018) for implementing distributed actors.
Recently, Bellemare et al. (2017) showed that the distribution over returns, whose expectation is the value function, obeys a distributional Bellman equation. Although the idea of estimating a distribution over returns has been revisited before (Sobel, 1982; Morimura et al., 2010), Bellemare et al. demonstrated that this estimation alone was enough to achieve state-of-the-art results on the Atari 2600 benchmarks. Crucially, this technique achieves these gains by directly improving updates for the critic.

## 2 BACKGROUND

In this work we consider a standard reinforcement learning setting wherein an agent interacts with an environment in discrete time. At each timestep t the agent makes observations xt P X , takes actions at P A, and receives rewards rpxt, atq P R. Although we will in general make no assumptions about the inputs X , we will assume that the environments considered in this work have real-valued actions A “ R d.
In this standard setup, the agent’s behavior is controlled by a policy π : X Ñ A which maps each observation to an action. The state-action value function, which describes the expected return conditioned on first taking action a P A from state x P X and subsequently acting according to π, is defined as

(1)

and is commonly used to evaluate the quality of a policy. While it is possible to derive an updated policy directly from Qπ, such an approach typically requires maximizing this function with respect to a and is made complicated by the continuous action space. Instead we will consider a parameterized policy πθ and maximize the expected value of this policy by optimizing Jpθq “ ErQπθ px, πθpxqqs. By making use of the deterministic policy gradient theorem (Silver et al., 2014) one can write the gradient of this objective as

(2)

where ρ is the state-visitation distribution associated with some behavior policy. Note that by letting the behavior policy differ from π we are able to empirically evaluate this gradient using data gathered off-policy.
While the exact gradient given by (2) assumes access to the true value function of the current policy, we can instead approximate this quantity with a parameterized critic Qwpx, aq. By introducing the Bellman operator

(3)

whose expectation is taken with respect to the next state x1, we can minimize the temporal difference (TD) error, i.e. the difference between the value function before and after applying the Bellman update. Typically the TD error will be evaluated under separate target policy and value networks, i.e. networks with separate parameters pθ1,w1q, in order to stabilize learning. By taking the two- norm of this error we can write the resulting loss as

(4)

In practice we will periodically replace the target networks with copies of the current network weights. Finally, by training a neural network policy using the deterministic policy gradient in (2) and training a deep neural to minimize the TD error in (4) we obtain the Deep Deterministic Policy Gradient (DDPG) algorithm (Lillicrap et al., 2016). Here a sample-based approximation to these gradients is employed by using data gathered in some replay table.

## 3 DISTRIBUTED DISTRIBUTIONAL DDPG

이 작업에서 취한 접근법은 DDPG 알고리즘에서 시작하여 여러 가지 향상된 기능이 포함되어 있습니다. 이 절에서 자세히 설명 할 이러한 확장에는 distributional critic update, distributed parallel actors, N-step return 및 prioritization of the experience replay(PER) 지정이 포함됩니다.

첫째, 그리고 아마도 가장 결정적으로 우리는 Bellemare et al.에서 소개 된 분배 비평가의 포함을 고려한다. (2017). 분산 갱신을 도입하기 위해, Qπ px, aq "E Zπ px, aq. 분산 Bellman 연산자는 다음과 같이 정의 할 수 있습니다.

The approach taken in this work starts from the DDPG algorithm and includes a number of enhancements. These extensions, which we will detail in this section, include a distributional critic update, the use of distributed parallel actors, N-step returns, and prioritization of the experience replay.
First, and perhaps most crucially, we consider the inclusion of a distributional critic as introduced in Bellemare et al. (2017). In order to introduce the distributional update we first revisit (1) in terms of the return as a random variable Zπ , such that Qπ px, aq “ E Zπ px, aq. The distributional Bellman operator can be defined as

(5)

여기서 평등은 확률 변수의 확률 법칙에 관한 것이다. 이 기대는 전이 역학뿐만 아니라 Z의 분포와 관련하여 취해진 다.
이 연산자의 정의가 (3)에서 정의 된 표준 Bellman 연산자와 매우 유사하게 보이지만, 함수가 작동하는 유형이 다릅니다. 분산 형은 상태 - 액션 쌍에서 분포로 매핑하는 함수를 취하고 같은 형식의 함수를 반환합니다. 위에 소개 된 배우 - 비평가 아키텍처의 맥락에서이 함수를 사용하려면이 분포를 매개 변수화하고 식 4와 비슷한 손실을 정의해야합니다.

where equality is with respect to the probability law of the random variables; note that this expecta- tion is taken with respect to distribution of Z as well as the transition dynamics.
While the definition of this operator looks very similar to the canonical Bellman operator defined in (3), it differs in the types of functions it acts on. The distributional variant takes functions which map from state-action pairs to distributions, and returns a function of the same form. In order to use this function within the context of the actor-critic architecture introduced above, we must parameterize this distribution and define a loss similar to that of Equation 4. We will write the loss as

(6)

두 분포 사이의 거리를 측정하는 일부 메트릭 d에 대해서. 이 알고리즘의 성능에 중요한 영향을 미칠 수있는 두 가지 구성 요소는 Zw에 사용되는 특정 매개 변수화와 분산 TD 오류를 측정하는 데 사용되는 메트릭 d입니다. 두 경우 모두 부록 A에 자세히 설명 할 것입니다. 다음에 나오는 실험에서 우리는 그 섹션에서 상세히 설명한 범주 형 분포를 사용할 것입니다.
방정식 2의 액터 업데이트 내에 동작 값 분포를 포함시킴으로써이 분배 정책 그라디언트 알고리즘을 완성 할 수 있습니다. 이것은 동작 값 분포에 대한 기대치를 취함으로써 이루어집니다.

for some metric d that measures the distance between two distributions. Two components that can have a significant impact on the performance of this algorithm are the specific parameterization used for Zw and the metric d used to measure the distributional TD error. In both cases we will give further details in Appendix A; in the experiments that follow we will use the Categorical distribution detailed in that section.
We can complete this distributional policy gradient algorithm by including the action-value distri- bution inside the actor update from Equation 2. This is done by taking the expectation with respect to the action-value distribution, i.e.

(7)

(Algorithm)

이전과 마찬가지로이 업데이트는 외부 기대치를 샘플 기반 근사치로 대체하여 경험적으로 평가할 수 있습니다.
다음으로, 우리는 TD 오류를 추정 할 때 N-step 리턴을 사용하는 DDPG 갱신에 대한 수정을 고려합니다. Bellman 연산자를 N-step 변형으로 대체하는 것으로 볼 수 있습니다.

As before, this update can be empirically evaluated by replacing the outer expectation with a sample- based approximation.
Next, we consider a modification to the DDPG update which utilizes N -step returns when estimating the TD error. This can be seen as replacing the Bellman operator with an N -step variant

(8)

여기서 기대는 N-step 전이 역학에 관한 것이다. Lillirap et al. (2016), N-step 복귀는 많은 정책 기울기 알고리즘 (예 : Mnih et al., 2016)뿐만 아니라 Q- 학습 변형 (Hessel et al., 2017)의 맥락에서 널리 사용된다. 이 수정은 분포 평론가를 업데이트 할 때이를 사용하기 위해 분산 Bellman 연산자와 유사하게 적용될 수 있습니다.
마지막으로 경험 수집 프로세스를 배포하기 위해 표준 교육 절차를 수정합니다. 방정식 (2,4)에서 배우와 비평가의 업데이트는 일부 주정부 방문 분포 ρ의 샘플링에만 전적으로 의존한다는 점에 유의하십시오. 우리는 K 개의 독립적 인 액터를 사용하여이 프로세스를 병렬 처리 할 수 ​​있습니다. 각각의 액터는 동일한 리플레이 테이블에 씁니다. 그런 다음 학습자 프로세스는 크기 R의 일부 재생 표에서 샘플링하고이 데이터를 사용하여 필요한 네트워크 업데이트를 수행 할 수 있습니다. 추가적으로 샘플링은 Schaul et al.에서와 같이 비 균일 우선 순위 pi를 사용하여 구현 될 수있다. (2016). 이를 위해서는 중요도 샘플링을 사용해야하며 비평가 업데이트를 1 {Rpi}로 가중치를 적용하여 구현해야합니다. 우리는 ApeX 프레임 워크 (Horgan et al., 2018)를 사용하여이 절차를 구현하고 자세한 내용은 독자에게 문의하십시오.
위에서 언급 한 모든 수정 사항을 포함하는 D4PG 알고리즘의 알고리즘 의사 코드는 알고리즘 1에서 볼 수 있습니다. 여기서 배우 및 비평 매개 변수는 ADAM을 사용하여 온라인으로 조정되는 학습 속도 αt 및 βt의 확률적인 그래디언트 강하를 사용하여 업데이트됩니다 (Kingma & Ba, 2015). 이 의사 코드는 학습 과정에 중점을 두는 반면, 병렬로 재생 테이블을 데이터로 채우는 액터 프로세스의 코드도 표시합니다.

where the expectation is with respect to the N -step transition dynamics. Although not used by Lilli- crap et al. (2016), N -step returns are widely used in the context of many policy gradient algorithms (e.g. Mnih et al., 2016) as well as Q-learning variants (Hessel et al., 2017). This modification can be applied analogously to the distributional Bellman operator in order to make use of it when updating the distributional critic.
Finally, we also modify the standard training procedure in order to distribute the process of gathering experience. Note from Equations (2,4) that the actor and critic updates rely entirely on sampling from some state-visitation distribution ρ. We can parallelize this process by using K independent actors, each writing to the same replay table. A learner process can then sample from some replay table of size R and perform the necessary network updates using this data. Additionally sampling can be implemented using non-uniform priorities pi as in Schaul et al. (2016). Note that this requires the use of importance sampling, implemented by weighting the critic update by a factor of 1{Rpi. We implement this procedure using the ApeX framework (Horgan et al., 2018) and refer the reader there for more details.
Algorithm pseudocode for the D4PG algorithm which includes all the above-mentioned modifica- tions can be found in Algorithm 1. Here the actor and critic parameters are updated using stochastic gradient descent with learning rates, αt and βt respectively, which are adjusted online using ADAM (Kingma & Ba, 2015). While this pseudocode focuses on the learning process, also shown is pseu- docode for actor processes which in parallel fill the replay table with data.


그림 1 : 각 도메인에 사용 된 아키텍처 변형. 가장 왼쪽에있는 세트는 표준 제어 및 조작 도메인에 사용되는 액터 네트워크와 비평 몸통을 보여줍니다. 비판 몸체의 출력을 관련 분포 (예 : 그림 A의 오른쪽 부분은 파 쿠르 도메인에서 사용한 아키텍처를 비슷하게 나타냅니다.

Figure 1: Architectural variants used for each domain. The left-most set illustrates the actor network and critic torso used for the standard control and manipulation domains. The full critic architecture is completed by feeding the output of the critic torso into a relevant distribution, e.g. the categorical distribution, as defined in Section A. The right half of the figure similarly illustrates the architecture used by the parkour domains.

## 4 RESULTS

이 절에서는 다양한 연속적인 제어 작업을 통해 D4PG 알고리즘의 성능을 설명합니다. 그렇게하기 위해 각 환경에서 학습 절차를 실행하고 주기적으로 정책을 스냅 샷하여 탐색 노이즈없이 테스트합니다. 우리는 기본적으로 벽시계 시간의 함수로 성능에 관심을 갖지만 데이터 효율을 조사 할 것입니다. 가장 흥미롭게도 과학적 관점에서 D4PG 알고리즘의 구성 요소를 개별적으로 제거하여 특정 기여도를 결정하는 여러 가지 제거 작업을 수행합니다.
첫째, 배포 업데이트를 사용하거나 사용하지 않고 실험합니다. 이 설정에서 우리는 예비 실험에서 가우스 혼합의 사용이 다른 작업에서 하이퍼 매개 변수 값과 관련하여 악화되었고 덜 안정적이라는 것을 발견 했으므로 범주 형 분포의 사용에 초점을 맞 춥니 다. 이러한 실행의 선택은 부록 C에서 찾을 수 있습니다. 모든 작업 (나중에 소개 할 항목 제외)에서 범주 형 분포에 51 개의 원자를 사용합니다. 다음에서는이 알고리즘의 비 분산 변형을 분산 DDPG (D3PG)라고합니다.

다음으로 이러한 알고리즘 변형의 우선 순위 및 비 우선 순위 버전을 고려합니다. 우선 순위가없는 변형의 경우 전환이 재생에서 균일하게 샘플링됩니다. 우선 순위가있는 변수의 경우 D3PG의 경우 재생에서 샘플링하기 위해 절대 TD 오류를 사용하고 D 섹션에서 설명한대로 D4PG의 경우 절대 분산 TD 오류를 사용합니다. 또한 궤도 길이 N P t1, 5u를 변경합니다.

모든 실험에서 우리는 크기 R "1 106의 재생 표를 사용하고 고정 된 가우스 잡음 εN p0, 1q를 현재 온라인 정책에 추가하는 행동 정책 만 고려합니다. 모든 실험에서 우리는 ε = 0.3의 값을 사용한다. 우리는 (Lillicrap et al., 2016)에 의해 제안 된 바와 같이 Ornstein-Uhlenbeck 과정에서 추출 된 상호 연관된 잡음을 실험했지만, 우리는 이것이 불필요하고 성능에 추가되지 않는다는 것을 발견했습니다. 모든 알고리즘에 대해 액터와 비평가 업데이트에 대한 학습 속도를 동일한 값으로 초기화합니다. 다음 섹션에서는이 값이 α0 "β0"1 10 4에 해당하는 간단한 제어 문제 모음을 제시합니다. 우리가 α0 "β0"5 10 5의 작은 값으로 설정 한 다음과 같은 더 어려운 문제에 대해서. 마찬가지로 컨트롤 스위트의 경우 M "256의 배치 크기를 사용하고 이후의 모든 문제에 대해서는 M"512로 증가시킵니다.

In this section we describe the performance of the D4PG algorithm across a variety of continu- ous control tasks. To do so, in each environment we run our learning procedure and periodically snapshot the policy in order to test it without exploration noise. We will primarily be interested in the performance as a function of wall clock time, however we will also examine the data effi- ciency. Most interestingly, from a scientific perspective, we also perform a number of ablations which individually remove components of the D4PG algorithm in order to determine their specific contributions.
First, we experiment with and without distributional updates. In this setting we focus on use of a categorical distribution as we found in preliminary experiments that the use of a mixture of Gaus- sians performed worse and was less stable with respect to hyperparameter values across different tasks; a selection of these runs can be found in Appendix C. Across all tasks—except for one which we will introduce later—we use 51 atoms for the categorical distribution. In what follows we will refer to non-distributional variants of this algorithm as Distributed DDPG (D3PG).

Next, we consider prioritized and non-prioritized versions of these algorithm variants. For the non- prioritized variants, transitions are sampled from replay uniformly. For prioritized variants we use the absolute TD-error to sample from replay in the case of D3PG, and for D4PG we use the absolute distributional TD-error as described in Section A. We also vary the trajectory length N P t1, 5u.

In all experiments we use a replay table of size R “ 1 ˆ 106 and only consider behavior policies which add fixed Gaussian noise εN p0, 1q to the current online policy; in all experiments we use a value of ε “ 0.3. We experimented with correlated noise drawn from an Ornstein-Uhlenbeck process, as suggested by (Lillicrap et al., 2016), however we found this was unnecessary and did not add to performance. For all algorithms we initialize the learning rates for both actor and critic updates to the same value. In the next section we will present a suite of simple control problems for which this value corresponds to α0 “ β0 “ 1 ˆ 10 ́4 ; for the following, harder problems we set this to a smaller value of α0 “ β0 “ 5 ˆ 10 ́5. Similarly for the control suite we utilize a batch size of M “ 256 and for all subsequent problems we will increase this to M “ 512.

### 4.1 STANDARD CONTROL SUITE

먼저 MuJoCo 물리 시뮬레이터 (Todorov et al., 2012)에서 개발 된 벤치 마크 태스크 모음 (Tassa et al., 2018)을 사용하여 여러 가지 간단한 물리적 제어 태스크의 성능을 평가하는 것을 고려합니다. 각 작업은 정확히 1000 단계로 실행되며 특정 작업에 따라 즉각적인 조밀 한 보상 rt P r0, 1s 또는 희소 보상 rt P t0, 1u를 제공합니다. 각 도메인에 대해 에이전트에 제공되는 입력은 물리적 상태, 관절 각 등으로 구성된 합리적으로 낮은 차원의 관측으로 구성됩니다. 이러한 관측 범위는 6 - 60 차원이지만, 작업의 어려움은 즉시 연관되지 않습니다 차원 적으로 예를 들어, 아크로뱃은이 제품군에서 가장 낮은 차원의 작업 중 하나입니다. 제어 가능성의 수준으로 인해 다른 차원 높은 과제보다 학습하기가 훨씬 어려울 수 있습니다. 이러한 도메인에 대한 설명은 그림 9를 참조하십시오. 자세한 내용은 부록 D를 참조하십시오.

We first consider evaluating performance on a number of simple, physical control tasks by utilizing a suite of benchmark tasks (Tassa et al., 2018) developed in the MuJoCo physics simulator (Todorov et al., 2012). Each task is run for exactly 1000 steps and provides either an immediate dense reward rt P r0, 1s or sparse reward rt P t0, 1u depending on the particular task. For each domain, the inputs presented to the agent consist of reasonably low-dimensional observations, many consisting of physical state, joint angles, etc. These observations range between 6 and 60 dimensions, however note that the difficulty of the task is not immediately associated with its dimensionality. For example the acrobot is one of the lowest dimensional tasks in this suite which, due to its level of controllability, can prove much more difficult to learn than other, higher dimensional tasks. For an illustration of these domains see Figure 9; see Appendix D for more details.

이 실험에서 알고리즘을 위해 우리는 그림 1에 주어진 형태의 배우 및 비평가 아키텍처를 고려하고 각 실험에서 K "32 개의 액터를 사용합니다. 그림 2는 D4PG의 성능과 전체 제어 작업에 걸친 다양한 절삭량을 보여줍니다. 이 플롯 세트는 상당히 분주하지만 알고리즘 성능에 대한 일반적인 아이디어를 얻을 수있는 광범위한 작업으로 사용됩니다. 더 어려운 영역에 대한 나중의 실험은 알고리즘 간의 차이점을 자세히 보여줍니다. 여기서 표준 (비 분산) DDPG 알고리즘을 기준선으로 비교하여 점선으로 표시된 검은 선으로 표시합니다. 이것은이 백서에서 제안 된 모든 개선점을 제거하고 가장 단순한 영역 인 Cartpole (Swingup)을 제외하고는 다른 모든 방법보다 성능이 떨어지는 것을 볼 수 있습니다. 작업의 난이도가 높아짐에 따라 이러한 성능 격차가 더욱 심해지므로 향후 실험을 위해이 줄을 줄거리에서 제거 할 것입니다.

다음으로, 모든 작업에서 최상의 성능은 전체 D4PG 알고리즘 (보라색 및 굵은 체로 표시)에 의해 얻어지는 것을 볼 수 있습니다. 여기에서 우리는 N "5의 더 긴 unroll 길이가 균등하게 더 우수하다는 것을 알 수 있습니다. (실선으로 이것을 나타냅니다.) 그리고 특히 D3PG와 D4PG 모두에서 N"1 "의 풀 길이 (점선으로 표시)가 때때로 불안정을 초래한다. 이것은 Cheetah (Walk) 및 Cartpole (Swingup Sparse) 작업에서 특히 명확합니다.

다음으로 가장 큰 이득은 배포 비평가 업데이트가 포함되어 있기 때문입니다. 특히 가장 어려운 작업에 도움이됩니다. 인간형 (실행)과 Acrobot. 매니퓰레이터는이 태스크 스위트 중에서도 상당히 어렵습니다. 여기서는 D3PG 및 D4PG 변형이 거의 동일한 성능을 얻음에도 불구하고 분산 업데이트를 포함하면 다른 태스크만큼 도움이되지 않습니다. 우선 순위 지정 사용과 관련하여 D4PG의 성능에 크게 기여하지 않는 것으로 보입니다. 그러나 D3PG의 경우는 그렇지 않습니다. 대부분의 작업에서 우선 순위 지정을 포함하여 크게 도움이됩니다.

For algorithms in these experiments we consider actor and critic architectures of the form given in Figure 1 and for each experiment we use K “ 32 actors. Figure 2 shows the performance of D4PG and its various ablations across the entire suite of control tasks. This set of plots is quite busy, however it serves as a broad set of tasks with which we can obtain a general idea of the algorithms performance. Later experiments on harder domains look more closely at the difference between algorithms. Here we also compare against the canonical (non-distributed) DDPG algorithm as a baseline, shown as a dotted black line. This removes all the enhancements proposed in this paper, and we can see that except on the simplest domain, Cartpole (Swingup), it performs worse than all other methods. This performance disparity worsens as we increase the difficulty of tasks, and hence for further experiments we will drop this line from the plot.

Next, across all tasks we see that the best performance is obtained by the full D4PG algorithm (shown in purple and bold). Here we see that the longer unroll length of N “ 5 is uniformly better (we show these as solid lines), and in particular we sometimes see for both D3PG and D4PG that an unroll length of N “ 1 (shown as dashed lines) can occasionally result in instability. This is especially apparent in the Cheetah (Walk) and Cartpole (Swingup Sparse) tasks.

The next biggest gain is arguably due to the inclusion of the distributional critic update, where it is particularly helpful on the hardest tasks e.g. Humanoid (Run) and Acrobot. The manipulator is also quite difficult among this suite of tasks, and here we see that the inclusion of the distributional update does not help as much as in other tasks, although note that here the D3PG and D4PG variants obtain approximately the same performance. As far as the use of prioritization is concerned, it does not appear to contribute significantly to the performance of D4PG. This is not the case for D3PG, however, which on many tasks is helped significantly by the inclusion of prioritization.

### 4.2 MANIPULATION

다음으로 우리는 D4PG 에이전트가 능숙한 조작을 배우는 능력을 강조하기 위해 고안된 일련의 작업을 고려합니다. 이 형식의 작업은 제어 작업의 높은 차원, 간헐적 인 접촉 역학 및 조작자의 잠재적 인 과소 작동 등과 같은 여러 가지 이유로 인해 어려울 수 있습니다.
여기서 우리는 22 자유도를 제어하는 ​​13 개의 액츄에이터로 구성된 MuJoCo 내에 구현 된 시뮬레이션 된 핸드 모델을 사용합니다. 이러한 실험을 위해 손목 부위는 축 방향으로 회전 할 수있는 공간의 고정 된 위치에 부착됩니다. 특히 이것은 손으로 물건을 집어 들고 손바닥으로 위로 회전하여 조작 할 수 있습니다. 우리는 먼저 임의의 높이에서 실린더를 손으로 떨어 뜨리는 작업을 고려합니다. 작업의 목표는 떨어지는 실린더를 잡는 것입니다. 다음 작업에서는 상담원이 탁상에서 물건을 집어서 목표 위치와 방향으로 움직여야합니다. 최종 작업은 타겟 오리엔테이션과 일치시키기 위해 넓은 실린더를 직접 회전시켜야하는 작업입니다. 모델과 작업에 대한 자세한 내용은 부록 E를 참조하십시오. 이러한 작업을 위해 우리는 이전 섹션과 K "64 배우와 동일한 네트워크 아키텍처를 사용합니다.
그림 3에서 우리는 다시 D4PG 알고리즘을 구성 요소의 제거와 비교합니다. 여기서 우리는 알고리즘을 맨 위 줄의 N "1과 맨 아래 줄의 N"5 사이에 분할하고 특히 모든 알고리즘에서 N "5가 균등하게 더 우수하다는 것을 알 수 있습니다. 모든 작업에 대해 전체 D4PG 알고리즘은 다른 레벨보다 더 높은 수준에서 수행됩니다. 이것은 N "5 경우에서 특히 명백하다. 전반적으로 우선 순위 사용은 결코 D4PG에 해를 끼치 지 않는 것처럼 보이지만 제한된 추가 가치로 보입니다. 흥미롭게도 D3PG 변형이 반드시 해당되는 것은 아닙니다 (배포 업데이트가없는 경우). 여기서 우선 순위 지정이 때때로 D3PG의 성능에 악영향을 미친다는 것을 알 수 있습니다.이 알고리즘은 알고리즘이 불안정해질 수있는 N "1 경우에 매우 쉽게 볼 수 있으며 Pickup 및 Orient 작업의 경우 완전히 배울 수 없습니다.

Next, we consider a set of tasks designed to highlight the ability of the D4PG agent to learn dexterous manipulation. Tasks of this form can prove difficult for many reasons, most notably the higher dimensionality of the control task, intermittent contact dynamics, and potential under-actuation of the manipulator.
Here we use a simulated hand model implemented within MuJoCo, consisting of 13 actuators which control 22 degrees of freedom. For these experiments the wrist site is attached to a fixed location in space, about which it is allowed to rotate axially. In particular this allows the hand to pick up objects, rotate into a palm-up position, and manipulate them. We first consider a task in which a cylinder is dropped onto the hand from a random height, and the goal of the task is to catch the falling cylinder. The next task requires the agent to pick up an object from the tabletop and then maneuver it to a target position and orientation. The final task is one wherein a broad cylinder must be rotated in- hand in order to match a target orientation. See Appendix E for further details regarding both the model and the tasks. For these tasks we use the same network architectures as in the previous section as well as K “ 64 actors.
In Figure 3 we again compare the D4PG algorithm against ablations of its constituent components. Here we split the algorithms between N “ 1 in the top row and N “ 5 in the bottom row, and in particular we can see that across all algorithms N “ 5 is uniformly better. For all tasks, the full D4PG algorithm performs either at the same level or better than other ablations; this is particularly apparent in the N “ 5 case. Overall the use of priorization never seems to harm D4PG, however it does appear to be of limited additional value. Interestingly this is not necessarily the case with the D3PG variant (i.e. without distributional updates). Here we can see that prioritization sometimes harms the performance of D3PG, and this is very readily seen in the N “ 1 case where the algorithm can either become unstable, or in the case of the Pickup and Orient task it completely fails to learn.

### 4.3 PARKOUR

마지막으로, 우리는 (Heess et al., 2017)에 의해 소개 된 파 쿠르 영역을 고려한다. 이 설정에서 에이전트는 정방향 이동에 대해 보상을받는 단순한 로봇 보행기를 제어하지만 임의로 샘플링 된 여러 장애물로 인해 방해받습니다. 시각화에 대해서는 그림 4를 참조하고 자세한 내용은 이전 작업을 참조하십시오. 첫 번째 실험은 2 차원 워커, 즉 워커가 수평 및 수직으로 움직일 수 있지만 고정 된 깊이 위치로 제한되는 도메인을 고려합니다. 이 영역에서 에이전트에게 제시되는 장애물로는 바닥 표면의 틈새, 뛰어 넘어야하는 장애물, 그리고 그 위나 아래에서 실행할 수있는 플랫폼이 있습니다. 에이전트는 팔다리의 각도와이 양의 다른 함수에 상응하는 고유 감수 (prorioceptive) 관찰 xproprio P R19를 제공받습니다. 다가오는 지형의 깊이 맵과 같은 특징을 포함하는 xterrain P R101 관측치에 대한 액세스도 제공됩니다. 이러한 입력을 수용하기 위해 그림 1에 지정된 네트워크 아키텍처를 사용합니다. 특히 스택 지형 정보를 처리하여 더 적은 수의 숨겨진 유닛으로 줄이기 전에 피드 포워드 (feed-forward) 레이어를 사용하여 추가 처리를 위해 프로 포리 취성 정보와 연결합니다. 이 영역의 동작은 토크 제어 및 P R6의 형태를 취합니다.
이 설정에서 D4PG 알고리즘의 성능을 조사하기 위해 이전 섹션의 제거를 고려하고 이전 페이퍼 (Heess et al., 2017)에서 사용 된 PPO 기준선을 추가로 도입했습니다. PPO를 포함한 모든 알고리즘에서 우리는 K "64 개의 액터를 사용합니다. 이 결과는 그림 5의 맨 위 줄에 표시되어 있습니다. 앞에서와 같이 N "1 및 N"5에 대해 별도로 성능을 조사한 결과, 언롤 길이가 길수록 성능이 향상된다는 것을 알 수 있습니다. 일관성을 위해 두 플롯의 PPO 기준선을 표시하지만 두 플롯 모두 이전 논문에서 제안한 설정과 길이 50의 설정을 사용하는 동일한 알고리즘입니다.
여기서 우리는 다른 알고리즘 구성 요소 각각에 대한 명확한 묘사와 분명한 이득을 다시 볼 수 있습니다. 가장 큰 이득은 우선 순위가 매겨지지 않은 D3PG / D4PG 변형을 비교함으로써 볼 수있는 배포 업데이트의 포함에서 비롯됩니다. D3PG의 우선 순위 사용에 따른 이점은 미미하지만 분산 업데이트를 고려할 때이 이득은 사라집니다. 마지막으로, 우리는 PPO베이스 라인과 비교할 때이 알고리즘이 N "1의 경우 D3PG와 유리하게 비교되지만 D4PG는 성능이 뛰어나다는 것을 알 수 있습니다. N "5 일 때 모든 알고리즘이 PPO를 능가한다.

Finally, we consider the parkour domain introduced by (Heess et al., 2017). In this setting the agent controls a simplified robotic walker which is rewarded for forward movement, but is impeded by a number of randomly sampled obstacles; see Figure 4 for a visualization and refer to the earlier work for further details. The first of our experiments considers a two-dimensional walker, i.e. a domain in which the walker is allowed to move horizontally and vertically, but is constrained to a fixed depth position. In this domain the obstacles presented to the agent include gaps in the floor surface, barriers it must jump over, and platforms that it can either run over or underneath. The agent is presented with proprioceptive observations xproprio P R19 corresponding to the angles of its limbs and other functions of these quantities. It is also given access to observations xterrain P R101 which includes features such as a depth map of the upcoming terrain, etc. In order to accommodate these inputs we utilize a network architecture as specified in Figure 1. In particular we make use of a stack of feed-forward layers which process the terrain information to reduce it to a smaller number of hidden units before concatenating with the proporioceptive information for further processing. The actions in this domain take the form of torque controls a P R6.
In order to examine the performance of the D4PG algorithm in this setting we consider the ablations of the previous sections and we have further introduced a PPO baseline as utilized in the earlier paper of (Heess et al., 2017). For all algorithms, including PPO, we use K “ 64 actors. These results are shown in Figure 5 in the top row. As before we examine the performance separately for N “ 1 and N “ 5, and again we see that the higher unroll length results in better performance. Note that we show the PPO baseline on both plots for consistency, but in both plots this is the same algorithm, with settings proposed in the earlier paper and unrolls of length 50.
Here we again see a clear delineation and clear gains for each of the other algorithm components. The biggest gain comes from the inclusion of the distributional update, which we can see by com- paring the non-prioritized D3PG/D4PG variants. We see marginal benefit to using prioritization for D3PG, but this gain disappears when we consider the distributional update. Finally, we can see when comparing to the PPO baseline that this algorithm compares favorably to D3PG in the case of N “ 1, however is outperformed by D4PG; when N “ 5 all algorithms outperform PPO.

다음으로, 그림 5의 하단에있는 그림에서 우리는 교육 시간뿐만 아니라 샘플 복잡성 측면에서도 성능을 고려합니다. 그렇게하기 위해, 각 알고리즘의 성능 대 액터 단계의 수, 즉 수집 된 전이 양을 플로팅합니다. 이 작업에서 고려한 평행 액터가 샘플 효율성을 위해 반드시 조정되지는 않기 때문에 이는 PPO에 더 유리할 수 있습니다. PPO가 D3PG의 비 우선 순위 버전을 능가 할 수 있고, 훈련 초기에 우선 순위 버전과 비교하여 유리하다고 생각합니다. 그러나 우선 순위가 부여 된 설정과 우선 순위가없는 설정 모두에서 배포 업데이트를 사용하면 성능이 크게 향상됩니다. 흥미롭게도 우리는 우선 순위가 매겨진 D4PG 버전이 아닌 경우 우선 순위 사용이별로 효과가 없다는 것을 알 수 있습니다. 실제로 N "5의 궤도에서, 우선 순위가없는 D4PG가 더 나은 성능을 나타내지 만 나중에는 이러한 성능 곡선이 수평을 이루는 것을 볼 수 있습니다. 벽 시계 시간과 관련하여 이러한 작은 차이는 다른 실행의 일정 계획에서 작은 대기 시간으로 인한 것일 수 있습니다.이 차이는 액터 단계와 관련하여 플롯에 비해 적음을 알 수 있습니다.
마지막으로 우리는 3 차원으로 움직일 수있는 휴머노이드 워커를 고려합니다. 이 영역의 장애물은 바닥에있는 틈새, 뛰어 넘어야하는 장애물 및 에이전트가 통과 할 수있는 틈이있는 벽으로 구성됩니다. 이 실험에서 우리는 이전 실험에서와 같은 네트워크 아키텍처를 사용합니다. 단, 관측치의 크기는 xproprio P R79 및 xterrain P R461입니다. 다시 동작은 토크 제어이지만 21 차원입니다. 이 작업에서는 범주 형 분포에 대한 원자 수를 51에서 101로 늘 렸습니다.이 변경은 해상도를 다른 작업과 대략 일치하도록 유지하기 위해 분포의 해상도 수준을 높입니다. 이것은 훨씬 더 어려운 제어 작업을 가진 이전의 파르코르 작업보다 훨씬 더 큰 차원의 문제입니다. 워커는 불안정하고 이전 실험에서보다 에이전트가 실패 할 수있는 더 많은 방법이 있습니다. 이 특정 도메인에 대한 결과는 그림 6에 표시되어 있으며 여기서 벽 시계 시간의 함수로 성능을 집중합니다. 이전에 가장 우수한 성능의 롤아웃 길이 N "5로 제한되었습니다.이 설정에서 우리는 명확한 묘사를 볼 수 있습니다 우선 가장 성능이 좋지 않은 PPO 결과와 우선 순위 버전에 약간의 차이가있는 D3PG 결과 및 마지막으로 D4PG 결과가 표시됩니다. 흥미롭게도 D4PG의 경우 2 차원 워커 사례에서와 같이 우선 순위 지정의 사용에는 아무런 이점도없는 것으로 보이며 두 버전 모두 거의 동일한 성능 곡선을 갖습니다. 사실 여기서의 성능은 아마도 이전 실험 세트의 성능보다 훨씬 더 가깝습니다.


Next, in the plots shown in Figure 5 on the bottom row we also consider the performance not just in terms of training time, but also in terms of the sample complexity. In order to do so we plot the performance of each algorithm versus the number of actor steps, i.e. the quantity of transitions collected. This is perhaps more favorable to PPO, as the parallel actors considered in this work are not necessarily tuned for sample efficiency. Here we see that PPO is able to out-perform the non-prioritized version of D3PG, and early on in training is favorable compared to the prioritized version, although this trails off. However, we still see significant performance gains by utilizing the distributional updates, both in a prioritized and non-prioritized setting. Interestingly we see that the use of prioritization does not gain much, if any over the non-prioritized D4PG version. Early in the trajectory for N “ 5, in fact, we see that the non-prioritized D4PG exhibits better performance, however later these performance curves level out. With respect to wall-clock time these small differences may be due to small latencies in the scheduling of different runs, as we see that this difference is less for the plot with respect to actor steps.
Finally we consider a humanoid walker which is able to move in all three dimensions. The obstacles in this domain consist of gaps in the floor, barriers that must be jumped over, and walls with gaps that allow the agent to run through. For this experiment we utilize the same network architecture as in the previous experiment, except now the observations are of size xproprio P R79 and xterrain P R461. Again actions are torque controls, but in 21 dimensions. In this task we also increased the number of atoms for the categorical distribution from 51 to 101. This change increases the level of resolution for the distribution in order to keep the resolution roughly consistent with other tasks. This is a much higher dimensional problem than the previous parkour task with a significantly more difficult control task: the walker is more unstable and there are many more ways for the agent to fail than in the previous experiment. The results for this particular domain are displayed in Figure 6, and here we concentrate on performance as a function of wall-clock time, restricted to the previously best performing roll-out length of N “ 5. In this setting we see a clear delineation between first the PPO results which are the poorest performing, the D3PG results where the prioritized version has a slight edge, and finally the D4PG results. Interestingly for D4PG we again see as in the two- dimensional walker case, the use of prioritization seems to have no benefit, with both versions have almost identical performance curves; in fact the performance here is perhaps even closer than that of the previous set of experiments.

## 5 DISCUSSION

이 작업에서는 D4PG 또는 분산 분산 DDPG 알고리즘을 도입했습니다. 당사의 주요 공헌은 DDPG 알고리즘에 대한 배포 업데이트를 포함하고 다중 배포 작업자를 모두 동일한 재생 테이블에 쓰는 것과 결합됩니다. 알고리즘에 대한 다른 많은 작은 변경 사항도 고려합니다. 이러한 간단한 수정은 모두 D4PG 알고리즘의 전반적인 성능에 기여합니다. 이러한 간단한 변경에서 가장 큰 성능 향상은 아마도 N-step 복귀를 사용한다는 것입니다. 흥미롭게도 우리는 우선 순위의 사용이 전반적인 D4PG 알고리즘 특히 덜 어려운 문제에 덜 중요하다는 것을 발견했습니다. 우선 순위 지정을 사용하면 D3PG 알고리즘의 성능을 확실히 높일 수 있었지만 불안정한 업데이트로 이어질 수도 있습니다. 이것은 조작 작업에서 가장 뚜렷했습니다.
마지막으로, 우리의 결과가 증명할 수있는 것처럼, D4PG 알고리즘은 여러 가지 매우 어려운 연속 제어 문제에 대해 최첨단 성능을 발휘할 수 있습니다.

In this work we introduced the D4PG, or Distributed Distributional DDPG, algorithm. Our main contributions include the inclusion of a distributional updates to the DDPG algorithm, combined with the use of multiple distributed workers all writing into the same replay table. We also consider a number of other, smaller changes to the algorithm. All of these simple modifications contribute to the overall performance of the D4PG algorithm; the biggest performance gain of these simple changes is arguably the use of N -step returns. Interestingly we found that the use of priority was less crucial to the overall D4PG algorithm especially on harder problems. While the use of prioritization was definitely able to increase the performance of the D3PG algorithm, we found that it can also lead to unstable updates. This was most apparent in the manipulation tasks.
Finally, as our results can attest, the D4PG algorithm is capable of state-of-the-art performance on a number of very difficult continuous control problems.